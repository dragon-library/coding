<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python Example on Library</title>
    <link>https://example.com/post/python/example/</link>
    <description>Recent content in Python Example on Library</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 29 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://example.com/post/python/example/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Python Web Scraping Part I</title>
      <link>https://example.com/post/python/example/python-web-scraping-part-1/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://example.com/post/python/example/python-web-scraping-part-1/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;featured.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;การทำ  &lt;strong&gt;Web Scraping&lt;/strong&gt;  เพื่อเก็บข้อมูลที่มีอยู่ใน Internet มาทำการวิเคราะห์ เพราะสำหรับการวิเคราะห์ข้อมูล ซึ่งเป็นงานของ Data Scientist แล้ว สิ่งที่สำคัญที่สุดก็คงจะเป็น  &lt;em&gt;ข้อมูล&lt;/em&gt;  นั่นแหละครับ&lt;/p&gt;

&lt;p&gt;นอกจากนี้ บริษัทใหญ่ ๆ หลาย ๆ แห่ง ก็ยังใช้ประโยชน์จากการทำ Web Scraping ในการเก็บรวบรวมความคิดเห็นของ social เพื่อเอามาวิเคราะห์ปรับปรุงผลิตภัณฑ์ของตัวเองได้อีกด้วย เช่น เก็บ complains ต่าง ๆ เกี่ยวกับผลิตภัณฑ์ในแต่ละชนิด หรือแม้กระทั่งการทำ sentiment analysis ด้วย technique NLP เพื่อดูว่า  &lt;em&gt;ความรู้สึก&lt;/em&gt;  ของลูกค้าหลังการใช้บริการเป็นอย่างไร จาก comment หรือ post ต่าง ๆ ในโลกออนไลน์แบบ real-time&lt;/p&gt;

&lt;p&gt;ในบทความนี้ก็เลยอยากจะเล่า flow ของการดูดข้อมูลแบบง่าย ๆ ซึ่งเหมาะสำหรับท่านที่พอจะเขียน Python ได้บ้างจนถึงชำนาญ และสนใจศึกษาเกี่ยวกับการทำ Web Scraping ครับ โดยในตัวอย่างนี้จะใช้เว็บ Wikipedia ซึ่งสามารถ scrape ได้ง่าย เหมาะกับผู้เริ่มต้นครับ&lt;/p&gt;

&lt;p&gt;โดยในบทความนี้จะนำเสนอเพียงแค่ basic เพื่อให้ผู้อ่านได้เห็นภาพรวมกว้าง ๆ ก่อน และจะกลับมาอธิบายลงลึกในแต่ละส่วนแยกกันไปในบทความถัด ๆ ไปอีกทีครับ&lt;/p&gt;

&lt;h2 id=&#34;get-started&#34;&gt;&lt;strong&gt;Get Started!&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;ก่อนอื่นจะขอแนะนำเครื่องมือที่ต้องใช้ก่อน จากนั้นก็จะแนะนำ flow ของการ scrape แบบง่าย ๆ พร้อมกับตัวอย่างโค้ดให้นำไปทดลองกันได้ง่าย ๆ ครับ&lt;/p&gt;

&lt;h2 id=&#34;ส-งท-ต-องเตร-ยม&#34;&gt;&lt;strong&gt;สึ่งที่ต้องเตรียม&amp;hellip;&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1. Python&lt;/strong&gt;  แน่นอนว่าถ้าจะเขียน Python script ก็ต้องมีโปรแกรมสำหรับ run script ก่อน ซึ่งตรงนี้สำหรับท่านที่ยังไม่มี Python ในเครื่อง ก็สามารถ install ได้จาก  &lt;a href=&#34;https://www.python.org/downloads/&#34; target=&#34;_blank&#34;&gt;link นี้&lt;/a&gt;  โดยแนะนำให้ใช้ version ตั้งแต่  &lt;code&gt;3.6&lt;/code&gt;  ขึ้นไปครับ&lt;/p&gt;

&lt;p&gt;แต่ถ้าหากไม่สะดวก install ลงในเครื่องที่ใช้อยู่ก็สามารถ execute Python script online ได้โดยใช้บริการ  &lt;strong&gt;Google Colab Notebook&lt;/strong&gt;  ของ Google ซึ่งสามารถแก้ไข และ execute Python script ทีละ block ได้ ผ่าน web browser จึงไม่จำเป็นต้อง install หรือ download อะไรลงในเครื่องทั้งสิ้นครับ&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;โดยโค้ดทั้งหมดที่ใช้ในบทความนี้จะแชร์ผ่าน Google Colab ด้วยเช่นกัน เนื่องจาก Data Scientist ส่วนใหญ่น่าจะคุ้นเคยกับ interface ลักษณะนี้ดี&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--KMRZMmU1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/1y2k0fb0qwubrbnfbj1u.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--KMRZMmU1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/1y2k0fb0qwubrbnfbj1u.png&#34; alt=&#34;google-colab.png&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;em&gt;ภาพ Google Colab Notebook&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. modules สำหรับการ scrape&lt;/strong&gt;  Python modules ที่จำเป็นต้องใช้ในบทความนี้ ได้แก่  &lt;code&gt;requests&lt;/code&gt;, และ  &lt;code&gt;lxml&lt;/code&gt;  โดยสามารถ install ผ่านโปรแกรม  &lt;code&gt;pip&lt;/code&gt;  ของ Python ได้จากคำสั่งต่อไปนี้&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;pip install requests lxml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;สำหรับท่านที่ใช้โค้ดบน Google Colab จะมี modules เหล่านี้ installed ไว้อยู่แล้ว ดังนั้นจึงไม่ต้องทำอะไรเพิ่มเติมครับ&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;เร-มก-นเถอะ&#34;&gt;&lt;strong&gt;เริ่มกันเถอะ!&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;code สำหรับบทความนี้จะแชร์ไว้ใน Google Colab ตาม  &lt;a href=&#34;https://colab.research.google.com/drive/1CxGihPm5if8YGOEjrVo9havGikqqQa6e&#34; target=&#34;_blank&#34;&gt;link นี้&lt;/a&gt;  นะครับ&lt;br /&gt;
ถ้าสะดวกสามารถเปิดโค้ดตามและทดลองแก้ไขและรันดูไปพร้อม ๆ กับที่อ่านบทความได้เลยนะครับ&lt;br /&gt;
โดยการคลิกแทบ  &lt;code&gt;File&lt;/code&gt;  ด้านบนของ Colab แล้วเลือก  &lt;code&gt;Save a copy in Drive...&lt;/code&gt;  ก็จะสามารถแก้ไขได้ และ save ลง Google Drive ให้อัตโนมัติครับ&lt;br /&gt;
ผู้อ่านสามารถรันโค้ดบน Colab ได้โดยการคลิกเลือกแต่ละ block แล้วกด  &lt;code&gt;Ctrl+Enter&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;python-web-scraping&#34;&gt;&lt;strong&gt;Python Web Scraping&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;สมมุติว่า เราต้องการจะดูดข้อมูลจาก  &lt;a href=&#34;https://th.wikipedia.org/wiki/%E0%B8%A3%E0%B8%B2%E0%B8%A2%E0%B8%8A%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B9%80%E0%B8%97%E0%B8%A8%E0%B8%9A%E0%B8%B2%E0%B8%A5%E0%B8%95%E0%B8%B3%E0%B8%9A%E0%B8%A5%E0%B9%83%E0%B8%99%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B9%80%E0%B8%97%E0%B8%A8%E0%B9%84%E0%B8%97%E0%B8%A2&#34; target=&#34;_blank&#34;&gt;https://th.wikipedia.org/wiki/รายชื่อเทศบาลตำบลในประเทศไทย&lt;/a&gt;  เพื่อเก็บรายชื่อของเทศบาลตำบลทั้งหมดในประเทศไทย ก็อาจจะแบ่งได้เป็น 2 ขั้นตอน คือ Scrape และ Extract&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--plYMCEN0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/ov86mmsfcop0q86h867m.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--plYMCEN0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/ov86mmsfcop0q86h867m.png&#34; alt=&#34;wikipedia-tambon.png&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;em&gt;Wikipedia Page ที่จะทำการ scrape&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Scrape&lt;/strong&gt;  ขั้นตอนแรกก็คือต้องเอาข้อมูลจากหน้าเว็บแบบดิบ ๆ เนี่ย ออกมาให้ได้ก่อน ซึ่งในเว็บอื่น ๆ ทั่ว ๆ ไป ก็จะมีความซับซ้อนหลาย ๆ อย่างที่ทำให้ไม่สามารถดึงออกมาได้ง่าย ๆ ซึ่งจะนำเสนอเทคนิคต่าง ๆ ที่ใช้รับมือแต่ละ cases ในบทความถัด ๆ ไปนะครับ&lt;br /&gt;
แต่เนื่องจากในส่วนนี้เราใช้ Web Wikipedia เป็นตัวอย่าง ซึ่งไม่ได้มีการป้องการ scrape หรือกลไกที่ซับซ้อนอะไรมาก เพราะงั้นสามารถส่ง  &lt;code&gt;GET request&lt;/code&gt;  ไปที่ URL โดยตรงตามโค้ดด้านล่างได้เลยครับ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import requests

url = &#39;https://th.wikipedia.org/wiki/รายชื่อเทศบาลตำบลในประเทศไทย&#39;
resp = requests.get(url)
print(resp)

# Output: &amp;lt;Response [200]&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;จากโค้ดด้านบนเราจะได้ object ของ  &lt;code&gt;Response&lt;/code&gt;  มาเก็บไว้ในตัวแปร  &lt;code&gt;resp&lt;/code&gt;&lt;br /&gt;
โดยเราสามารถเรียกดู content ของ response ที่ได้กลับมาได้โดยใช้  &lt;code&gt;resp.content&lt;/code&gt;  หรือ  &lt;code&gt;resp.text&lt;/code&gt;  แตกต่างกันเล็กน้อย คือ  &lt;code&gt;.content&lt;/code&gt;  จะให้ค่า content ที่เป็น bytes ออกมา ส่วน  &lt;code&gt;.text&lt;/code&gt;  จะ decode ข้อมูล bytes ออกเป็น string ให้ จึงสามารถแสดงตัวอักษรภาษาต่าง ๆ ให้ดูได้ เช่นภาษาไทย&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;print(resp.text[:5_000])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;คำสั่งด้านบนจะเรียกดูข้อมูลจำนวน 5,000 ตัวอักษรแรก (limit ไว้เพื่อไม่ให้มันรกเฉย ๆ)&lt;/p&gt;

&lt;p&gt;จะเห็นว่าเราได้โค้ด HTML ของหน้า page นั้น ๆ มาทั้งหมด&lt;br /&gt;
นั่นคือตอนนี้เราดึงข้อมูลดิบของหน้าเว็บนั้นมาอยู่ใน Python เรียบร้อยแล้ว&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Extract&lt;/strong&gt;  ต่อไปเราจะนำข้อมูลดิบที่ได้มา มากรองให้เหลือแค่ข้อมูลที่เราอยากได้เท่านั้น นั่นก็คือชื่อเทศบาลตำบล&lt;br /&gt;
โดยทั่วไปในการ extract ข้อมูล วิธีการที่จะใช้นั้นก็ขึ้นอยู่กับหน้าตาของข้อมูล เช่น เป็น HTML, เป็น JSON, หรือเป็น plain text ซึ่ง modules ใน Python ที่จะนำมาใช้กับข้อมูลแต่ละประเภทก็มีหลายอย่าง เช่น  &lt;code&gt;lxml&lt;/code&gt;,  &lt;code&gt;json&lt;/code&gt;,  &lt;code&gt;beautifulsoup&lt;/code&gt;, หรือ  &lt;code&gt;re&lt;/code&gt;&lt;br /&gt;
ในกรณีนี้ เนื่องจากข้อมูลดิบเราเป็น HTML ผู้เขียนจึงใช้  &lt;code&gt;lxml&lt;/code&gt;  เนื่องจาก  &lt;code&gt;lxml&lt;/code&gt;  มีฟังก์ชันที่ใช้แปลงข้อมูล HTML ให้กลายเป็น tree ซึ่งช่วยให้เราสามารถ extract แต่ละ element ใน HTML ได้ง่าย (เป็นความเห็นส่วนตัวนะครับ บางท่านอาจจะถนัดใช้ beautifulsoup มากกว่า ซึ่งอาจจะเขียนถึงในบทความถัด ๆ ไป)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import lxml.etree

content = resp.content
tree = lxml.etree.fromstring(content, parser=lxml.etree.HTMLParser())
print(tree)

# Output: &amp;lt;Element html at 0x7f34c830e948&amp;gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note: เราต้องระบุ  &lt;code&gt;parser&lt;/code&gt;  ให้เป็น  &lt;code&gt;HTMLParser&lt;/code&gt;  เนื่องจากจริง ๆ แล้ว  &lt;code&gt;lxml&lt;/code&gt;  สามารถใช้กับข้อมูลประเภทอื่นนอกจาก HTML ได้ด้วย&lt;/p&gt;

&lt;p&gt;พอเราแปลงข้อมูลเป็น tree แล้ว ต่อไปเราก็จะสามารถระบุตำแหน่งของ element ที่เก็บข้อมูลที่เราต้องการได้โดยใช้  &lt;code&gt;XPath&lt;/code&gt;&lt;br /&gt;
อธิบายแบบคร่าว ๆ  &lt;code&gt;XPath&lt;/code&gt;  ก็คล้าย ๆ กับ path ของ folder ต่าง ๆ ในเครื่องคอมพิวเตอร์ ที่เอาไว้ระบุตำแหน่งไฟล์หรือ folder ที่เราต้องการ โดยเริ่มจาก  &lt;em&gt;root&lt;/em&gt;  (C:/) แล้วก็ไปยัง folder ต่าง ๆ ตามลำดับ เช่น  &lt;em&gt;C:/Users/CopyPasteEng/Downloads&lt;/em&gt;  แต่  &lt;code&gt;XPath&lt;/code&gt;  ในที่นี้จะใช้ระบุตำแหน่งของ HTML element บนโค้ด code แทน&lt;br /&gt;
ตัวอย่าง  &lt;code&gt;XPath&lt;/code&gt;  เช่น  &lt;code&gt;//div[@id=&amp;quot;mw-content-text&amp;quot;]/div/p/text()&lt;/code&gt;  อันนี้จะแปลว่า&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. ให้เข้าไปหา element ประเภท div ที่มี id เป็น &amp;quot;mw-content-text&amp;quot;
2. จากนั้นเข้าต่อไปที่ child element ที่เป็นประเภท div
3. แล้วก็เข้าต่อไปที่ element ประเภท p
4. แล้วเอา content ที่เป็น text ทั้งหมดออกมา

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note:  &lt;code&gt;div[...]&lt;/code&gt;  คือการใส่เงื่อนไขในการเลือก  &lt;code&gt;div&lt;/code&gt;  นั้น ๆ เข้าไป ในกรณีด้านบน เงื่อนไขก็คือ attribute  &lt;em&gt;id&lt;/em&gt;  ต้องเท่ากับ  &lt;code&gt;mw-content-text&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;ทีนี้คำถามต่อมาก็คือจะเอา  &lt;code&gt;XPath&lt;/code&gt;  ที่บอกตำแหน่งของข้อมูลที่เราต้องการมาได้อย่างไร&lt;br /&gt;
คำตอบก็คือต้องดูจาก code ของ page ครับ ไม่มีทางอื่น 555&lt;br /&gt;
ซึ่งเครื่องมือหนึ่งที่สามารถช่วยให้แกะโค้ดของ web ต่าง ๆ ได้ง่าย ๆ และฟรี ก็คือ  &lt;strong&gt;Code Inspector&lt;/strong&gt;  บน  &lt;strong&gt;Google Chrome&lt;/strong&gt;  ครับ สามารถเปิดขึ้นมาได้โดยคลิกขวาที่หน้าเว็บแล้วเลือก  &lt;code&gt;Inspect&lt;/code&gt;  หรือกด  &lt;code&gt;F12&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;กลับมาที่เว็บตัวอย่างของเราครับ  &lt;code&gt;XPath&lt;/code&gt;  ของชื่อเทศบาลตำบลทั้งหมดที่เราต้องการก็คือ  &lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[2 &amp;lt;= position()]/tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]//a//text()&lt;/code&gt;* เราจึงสามารถที่จะ extract ข้อมูลออกจาก tree ได้ด้วยคำสั่งต่อไปนี้ ก็จะได้ชื่อของเทศบาลตำบลในประเทศไทยจาก Wikipedia มาเก็บเป็น list เอาไว้ได้ตามต้องการครับ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;xpath = &#39;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[2 &amp;lt;= position()]/tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]//a//text()&#39;
tambon_list = tree.xpath(xpath)
print(tambon_list)

# Output: [&#39;เทศบาลตำบลบ้านดู่&#39;, &#39;เทศบาลตำบลเวียงชัย&#39;, ...

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;*ขออนุญาตแก้ XPath ในตัวอย่างนี้เป็นแบบที่ยาวขึ้น เพื่อให้อธิบายได้ง่ายในบทความ part 2 นะครับ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;เพื่อให้บทความกระชับ ผู้จัดทำได้ละรายละเอียดบางส่วนเอาไว้&lt;br /&gt;
เพราะจริง ๆ แล้ว ทั้งในส่วนของการ scrape และ extract ก็มีรายละเอียดอีกมาก และมีเทคนิคที่อาจจะต้องใช้มากมายซึ่งแตกต่างกันตามแต่ละ website เช่น  &lt;code&gt;XPath&lt;/code&gt;  ที่อยู่ใน code นี่ได้มาได้ยังไง? จะทำอย่างไรกับบาง website ที่มีกลไกที่ป้องกันการ scraping หรือบาง website ที่มี format ขอข้อมูลเป็นลักษณะอื่น&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;เพื่อให้ผู้อ่านได้รับข้อมูลครบถ้วน ในบทความนี้จึงนำเสนอเพียงแค่ basic ให้ผู้อ่านได้เห็นภาพรวมกว้าง ๆ ก่อน และคิดว่าจะกลับมาอธิบายลงลึกในแต่ละส่วนแยกกันไปอีกทีครับ&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;คิดว่าจากบทความนี้ ผู้อ่านน่าจะได้เห็นภาพของการ scrape แบบง่าย ๆ ไปแล้วสำหรับการเริ่มต้น ในบทความถัดไปเราก็จะสามารถไปดูเคสที่ซับซ้อนกันต่อได้ครับ&lt;/p&gt;

&lt;p&gt;อย่างที่ได้กล่าวไปข้างต้นครับ ว่าในการวิเคราะห์ข้อมูลเพื่อทำนายเหตุการณ์ต่าง ๆ ให้ถูกต้องและแม่นยำสิ่งที่สำคัญที่สุดคือ  &lt;strong&gt;ข้อมูล&lt;/strong&gt;  ที่อัพเดตตลอดเวลา&lt;br /&gt;
ซึ่งเราอาจจะมอง Internet เป็นเหมือนแหล่งข้อมูลฟรี ที่เราสามารถ scrape มายังไงก็ได้  &lt;strong&gt;แต่ควรคำนึงถึงเรื่องของกฎหมายและความเหมาะสมด้วยครับ สิ่งสำคัญก็คือต้องไม่ทำให้ใครเดือดร้อน&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;และสำหรับท่านที่สนใจอยากทราบรายละเอียดของแต่ละขั้นตอนอย่างละเอียด ก็สามารถติดตามได้ในบทความที่จะลงต่อ ๆ ไปนะครับ&lt;/p&gt;

&lt;p&gt;หรือถ้ามีเรื่องไหนที่สนใจเพิ่มเติมสามารถ comment เอาไว้ได้นะครับ&lt;/p&gt;

&lt;p&gt;FB Page:  &lt;a href=&#34;https://www.facebook.com/CopyPasteEng&#34; target=&#34;_blank&#34;&gt;Copy Paste Engineer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;- ขอบคุณที่อ่านครับ -&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;part อื่น ๆ ใน series&lt;/strong&gt;&lt;br /&gt;
-  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-1-python-49ce&#34; target=&#34;_blank&#34;&gt;part 1: การดูดข้อมูลเบื้องต้น ด้วย Python&lt;/a&gt;&lt;br /&gt;
-  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-2-chrome-s-code-inspector-3ok6&#34; target=&#34;_blank&#34;&gt;part 2: Chrome&amp;rsquo;s Code Inspector&lt;/a&gt;&lt;br /&gt;
-  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-3-extract-xpath-18h&#34; target=&#34;_blank&#34;&gt;part 3: เทคนิคการ extract ข้อมูลด้วย XPath&lt;/a&gt;&lt;br /&gt;
-  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-4-scrape-7-scrape-4ko4&#34; target=&#34;_blank&#34;&gt;part 4: ทำไมถึง scrape บางเว็บไม่ได้??? 7 เทคนิคง่าย ๆ ให้ scrape เว็บส่วนใหญ่ได้ลื่นปรื๊ด&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Reference : &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-1-python-49ce&#34; target=&#34;_blank&#34;&gt;https://dev.to/copypasteengineer/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
