<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python Example on Library</title>
    <link>https://dragon-library.github.io/post/python/example/</link>
    <description>Recent content in Python Example on Library</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 May 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://dragon-library.github.io/post/python/example/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>การใช้งาน Python ควบคุม Google sheet</title>
      <link>https://dragon-library.github.io/post/python/example/python-control-google-sheet/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://dragon-library.github.io/post/python/example/python-control-google-sheet/</guid>
      <description>

&lt;p&gt;วันนี้ Stackpython จะมาบอกวิธีการดึงขอมูลหรือเเก้ไขปรับปรุง ข้อมูลใน Google sheet ด้วย Python&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Google Sheets&lt;/strong&gt; คือ App ในกลุ่มของ Google Drive ซึ่งเป็นนวัตกรรมของทาง Google ซึ่งมีลักษณะการทำงานที่คล้ายกับ Microsoft Excel มีการสร้าง Column Row สามารถใส่ข้อมูลลงไปใน Cell ได้และที่สำคัญคือไม่ต้องติดตั้งที่เครื่อง สามารถใช้งานบน Web ได้ โดย ไฟล์ที่เราทำนั้นจะถูกบันทึกไว้ใน Google Drive ทำให้สามารถเปิดใช้งานได้ ไม่ว่าจะอยู่ที่ใด&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;การเปิดช่องทางการเข้าถึง Google Sheet และกำหนดสิทธิ์ให้กับ Application ของเราที่ต้องการจะเข้าถึงข้อมูลผ่านตัว Google Sheet API มีวิธีดังต่อนี้&lt;/p&gt;

&lt;h1 id=&#34;เร-มสร-างโปรเจคท&#34;&gt;เริ่มสร้างโปรเจคท์&lt;/h1&gt;

&lt;p&gt;มาเริ่มสร้างโปรเจ็กต์ เพื่อเตรียมช่องทางการเข้าถึงข้อมูลใน Google Sheet ก่อนอื่นให้เข้า แพลตฟอร์ม Google Cloud และ เข้าเมนูเลือกไปที่ IAMและผู้ดูแลระบบ -&amp;gt; เลือกจัดการทรัพยากร&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1010/1*B-8JhFOxSPNzS_YvnKyYkw.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ไปที่สร้างโปรเจ็กต์ -&amp;gt; ตั้งชื่อ โปรเจ็กต์ และกดสร้าง เมื่้อเสร็จจะมีเมนูที่เราสร้าง เด้งขึ้นมามุมบนขวามือ&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/481/1*Xvvl-04_xd_9hB4NoDMB_w.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/573/1*ORIPGVLdBciSiLY6wNXkkw.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1857/1*2OEXIhG7UdnHU-UhXu2m8g.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;เข้าที่เมนูอีกครั้ง แล้วเลือกไปที่ API และบริการ -&amp;gt; เลือกหน้าแดชบอร์ด จากนั้นคลิกที่ เลือกโปรเจ็กต์ เเละทำการเลือกโปรเจ็กต์ที่เราสร้างไว้ -&amp;gt; ไปที่ข้อมูลรับรอง&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/684/1*NCkuG-2dECOjn0jF7OUeWQ.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/664/1*f9fB3ZUvhFvoK0KVRwyilg.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/646/1*IQcnaLGi5Zmh1IWupObhtg.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;เมื่ออยู่ในหน้า ข้อมูลเข้าสู่ระบบ เราจะสร้างข้อมูลรับรองเป็นบัญชีบริการ เลือกที่สร้างข้อมูลรับรอง -&amp;gt; บัญชีบริการ&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/977/1*WaTYoR-tLZiIWuaneKEDrQ.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;จากนั้นตั้งชื่อเเละกดสร้าง -&amp;gt; ให้เลือกบทบาทเป็นโปรเจ็กต์ ผู้เเก้ไข -&amp;gt; กดสร้างคีย์ และเลือก JSON มันจะโหลดไฟร์มาให้เราเก็บไว้ เมื่อกดปุ่มเสร็จสิ้น จะกลับมาที่หน้าหลัก ข้อมูลเข้าสู่ระบบ จะเห็นรายการ บัญชีบริการที่เราสร้างไว้ แสดงขึ้นมาด้านล่าง&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/603/1*ovcm3dpKY5aoyDVQqN6PSQ.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/60/1*esC5aZctoqTXXQg0Ax1C7w.jpeg?q=20&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/666/1*esC5aZctoqTXXQg0Ax1C7w.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/566/1*ugDF6u6bSYbb2oTZ-YFUzg.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1245/1*thceZKxNiBRuE3ubrRKR6g.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ให้ทำการแก้ไขชื่อไฟล์ JSON ที่โหลดออกมาเป็นชื่อ creds.json -&amp;gt;ให้ Copy ข้อมูลส่วน client_email ไว้ เพื่อจะนำไปตั้งสิทธิให้เข้าถึง Google Sheet ที่เราต้องการโดย การกดปุ่ม แชร์ที่ Google Sheet และใส่ข้อมูล email เป็น ข้อมูล client_email ส่วนที่เรา Copy มาและกดส่ง เพื่อ setApplication ให้มีสิทธิเข้าถึง Google Sheet นี้ได้&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/976/1*lSGFDrXKueBxUzup1rbjBg.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1373/1*Zb3v9LhlixBG_FLdHePO5w.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;จากนั้นให้ไป ไลบรารี ค้นหา Google Sheets API และ Google Drive API เพื่อเปิดใช้งาน -&amp;gt;ไปที่ Google Drive API กดสร้างข้อมูลเข้าสู้ระบบ และเลือก Google Drive API , เว็บเซิร์ฟเวอร์ , ข้อมูลแอปพลิเคชัน , ไม่ ฉันไม่ได้ใช้ แล้วกดฉันต้องใช้ข้อมูลรับรองใด -&amp;gt; กดเสร็จสิ้น&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/683/1*BVXwlChWgLsE0gZEhl6vUA.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/713/1*wWKcSYdI9QvkfVU7-VGqsA.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/60/1*902gITott4GzZPizfYXHrg.jpeg?q=20&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/718/1*902gITott4GzZPizfYXHrg.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/60/1*-Wd0W_b5x8Lr2-DSCKXC3A.jpeg?q=20&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/899/1*-Wd0W_b5x8Lr2-DSCKXC3A.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/782/1*mNWIXH-tLQ75_xqnXTkh-g.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;เข-ยนไพธอนด-งข-อม-ลจาก-google-sheet&#34;&gt;เขียนไพธอนดึงข้อมูลจาก Google Sheet&lt;/h1&gt;

&lt;p&gt;ขั้นตอนต่อไป เราจะมาเขียน Python ให้ดึงแก้ไขข้อมูลออกมาจาก Google Sheet กัน ก่อนอื่นต้องสร้าง virtualenvironment ใน folderที่เราเก็บไฟร์ที่โหลดมาไว้ (แนะนำให้สร้างใหม่เเละเก็บไว้ด้วยกัน) จากนั้นสร้าง file.py เพื่อใช้ในการเขียนโค้ด -&amp;gt; Install Library ที่จำเป็น oauth2client และ gspread ด้วยคำสั้ง pip install oauth2client , pip install gspread&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1124/1*3J4YoVhZIADMzm5bDyu5XA.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/746/1*UTSCogrqZM-IZSF9jj3Z6A.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/803/1*nNisxjig9TrEuEw5HPiTIA.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/790/1*P4burFRy-e6ji3--szg2xQ.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;โอเครเรามาดูโค้ดกัน(อันแรกเป็นการดึงข้อมูลมาแสดง)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import gspread
from oauth2client.service_account import ServiceAccountCredentials
from pprint import pprint
scope = [&amp;quot;https://spreadsheets.google.com/feeds&amp;quot;,&#39;https://www.googleapis.com/auth/spreadsheets&#39;,&amp;quot;https://www.googleapis.com/auth/drive.file&amp;quot;,&amp;quot;https://www.googleapis.com/auth/drive&amp;quot;]
cerds = ServiceAccountCredentials.from_json_keyfile_name(&amp;quot;cerds.json&amp;quot;, scope)
client = gspread.authorize(cerds)
sheet = client.open(&amp;quot;ชื่อไฟล์งาน&amp;quot;).worksheet(&#39;แผ่นที่จะเรียกเปิด&#39;) # เป็นการเปิดไปยังหน้าชีตนั้นๆ
data = sheet.get_all_records()  # การรับรายการของระเบียนทั้งหมด
pprint(data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ตัวอย่างผลลัพธ์&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1862/1*2GLtVjbNW6c4P8ZpF7mQMQ.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;(ต่อไปจะเป็นการแก้ไขข้อความ)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import gspread
from oauth2client.service_account import ServiceAccountCredentials
from pprint import pprint
scope = [&amp;quot;https://spreadsheets.google.com/feeds&amp;quot;,&#39;https://www.googleapis.com/auth/spreadsheets&#39;,&amp;quot;https://www.googleapis.com/auth/drive.file&amp;quot;,&amp;quot;https://www.googleapis.com/auth/drive&amp;quot;]
cerds = ServiceAccountCredentials.from_json_keyfile_name(&amp;quot;cerds.json&amp;quot;, scope)
client = gspread.authorize(cerds)
sheet = client.open(&amp;quot;ชื่อไฟล์งาน&amp;quot;).worksheet(&#39;แผ่นที่จะเรียกเปิด&#39;) # เป็นการเปิดไปยังหน้าชีตนั้นๆ
cell=sheet.cell(4,2).value
pprint(cell)
sheet.update_cell(4,2,&amp;quot;แก้ไข&amp;quot;)
cell=sheet.cell(4,2).value
pprint(cell)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ตัวอย่างผลลัพธ์&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1862/1*iN2NI0OPHKF-dNf85cEI8Q.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;นอกจากนี้ยังมีคำสั่งอีกมาก เช่น&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt; sheet.row_value(*) #ใส่เลขที่ต้องการแทน *­­­­­­­­ 
 sheet.col_value(*) #ใส่เลขที่ต้องการแทน *
 sheet.delete_row(*) #ใส่เลขที่ต้องการแทน *
 sheet.delete_columns(*) #ใส่เลขที่ต้องการแทน *
 sheet.insert_row([&amp;amp;],*)#ใส่เลขที่ต้องการแทน *และข้อความแทน &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ลองไปใช้กันดูนะครับ&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ขอบคุณมากครับที่เข้ามาอ่านบทความของทางเพจ Stackpython ทั้งนี้ หากสงสัยตรงไหนเกี่ยวกับเนื้อหาในบทความสามารถติดต่อสอบถามทางเพจเราได้เลยครับ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Reference : &lt;a href=&#34;https://medium.com/@stackpython/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B8%87%E0%B8%B2%E0%B8%99-python-%E0%B8%84%E0%B8%A7%E0%B8%9A%E0%B8%84%E0%B8%B8%E0%B8%A1-google-sheet-665a6dca077d&#34; target=&#34;_blank&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Python Web Scraping Part IV - ทำไมถึง scrape บางเว็บไม่ได้</title>
      <link>https://dragon-library.github.io/post/python/example/web-scraping-series-1/python-web-scraping-part-4/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://dragon-library.github.io/post/python/example/web-scraping-series-1/python-web-scraping-part-4/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;หลังจากที่ใน  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-2-chrome-s-code-inspector-3ok6&#34; target=&#34;_blank&#34;&gt;part ที่แล้ว&lt;/a&gt;  เราได้เรียนรู้วิธีการแกะ XPath ของแต่ละ element ออกมาจาก web browser แบบง่าย ๆ กันแล้ว แต่ทีนี้ในกรณีที่ข้อมูลที่เราสนใจมีหลายตำแหน่งมาก  &lt;strong&gt;เราคงไม่อยากเสียเวลานั่ง extract แต่ละ element ด้วยมือทั้งหมดใช่ไหมครับ&lt;/strong&gt;  (เพราะถ้าจะทำด้วยมือทั้งหมด ก็ก๊อปตัวข้อมูลมาตรง ๆ เลยสะดีกว่า 555)&lt;/p&gt;

&lt;p&gt;ท่านที่พอจะมีประสบการณ์การทำเว็บมาบ้าง พอเห็น XPath สัก 1 หรือ 2 อัน ก็อาจจะเดาได้แล้ว ว่า XPath ที่เหมาะสม ที่น่าจะครอบคลุมทุก ๆ element นั้น ควรจะเป็นอะไร แต่ถึงจะเดาไม่ออกก็ไม่เป็นไรครับ เพราะอย่างบาง website ที่ซับซ้อนขึ้นหน่อย ต่อให้มีประสบการณ์ก็คงเดาไม่ออกในทีแรกเช่นกัน&lt;/p&gt;

&lt;p&gt;สำหรับในบทความนี้ก็จะสอนวิธีหนึ่งที่สามารถทำได้ เพื่อหา XPath ที่  &lt;strong&gt;&amp;ldquo;น่าจะ&amp;rdquo;&lt;/strong&gt;  ครอบคลุมข้อมูลทั้งหมดที่เราต้องการมานะครับ ซึ่งเป็นวิธีที่ผู้เขียนเองก็ใช้งานอยู่จริงในการ scrape website ในหลาย ๆ ครั้งครับ (ที่บอกว่า  &lt;strong&gt;&amp;ldquo;น่าจะ&amp;rdquo;&lt;/strong&gt;  เนี่ย เพราะว่าในความเป็นจริงไม่มีใครรู้หรอกครับ ว่า website เป้าหมายนี้ ถูกเขียน หรือถูกสร้างมาอย่างไร แต่ด้วยวิธีดังกล่าว+หลักของสถิติ เราก็จะมั่นใจได้ในระดับหนึ่งว่า XPath ของเรามัน  &lt;strong&gt;&amp;ldquo;น่าจะ&amp;rdquo;&lt;/strong&gt;  ดีเพียงพอครับ)&lt;/p&gt;

&lt;p&gt;วิธีหนึ่งที่ผมทำก็คือ sample XPath ออกมาดู เอามา  &lt;strong&gt;สำรวจ&lt;/strong&gt;  เพิ่มอีกเยอะ ๆ ก็จะช่วยให้เห็นภาพมากขึ้นครับ จากนั้นเราก็จะใช้ syntax ของ XPath ในการ select และระบุ condition เพื่อให้ครอบคลุมกรณีที่เรา sample ออกมา ให้มากที่สุดครับ&lt;/p&gt;

&lt;h2 id=&#34;get-started&#34;&gt;Get Started!&lt;/h2&gt;

&lt;p&gt;ทบทวนสักเล็กน้อย ใน part นี้เราก็ยังอยู่กับเว็บ Wikipedia  &lt;a href=&#34;https://th.wikipedia.org/wiki/%E0%B8%A3%E0%B8%B2%E0%B8%A2%E0%B8%8A%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B9%80%E0%B8%97%E0%B8%A8%E0%B8%9A%E0%B8%B2%E0%B8%A5%E0%B8%95%E0%B8%B3%E0%B8%9A%E0%B8%A5%E0%B9%83%E0%B8%99%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B9%80%E0%B8%97%E0%B8%A8%E0%B9%84%E0%B8%97%E0%B8%A2&#34; target=&#34;_blank&#34;&gt;https://th.wikipedia.org/wiki/รายชื่อเทศบาลตำบลในประเทศไทย&lt;/a&gt;  กันเหมือนเดิมนะครับ โดยเป้าหมายคือ  &lt;strong&gt;หา XPath สำหรับ extract รายชื่อเทศบาลตำบลทั้งหมด&lt;/strong&gt;  ซึ่งใน  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-2-chrome-s-code-inspector-3ok6&#34; target=&#34;_blank&#34;&gt;part ที่แล้ว&lt;/a&gt;  เราใช้ Chrome Inspector ในการแกะ XPath ของ  &lt;strong&gt;&amp;ldquo;Element&amp;rdquo; หนึ่ง ๆ&lt;/strong&gt;  มาได้แบบง่าย ๆ แล้วนะครับ ในตอนนี้เรากำลังจะหา XPath ที่สามารถ extract ข้อมูลทั้งหมดได้  &lt;strong&gt;(คือครอบคลุมทุก element ไม่ใช่แค่ 1 element)&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;1-sample-xpath-มาเพ-ม&#34;&gt;1. Sample XPath มาเพิ่ม&lt;/h3&gt;

&lt;p&gt;อย่างในเว็บเป้าหมาย ถ้าลอง scroll เลื่อนขึ้นลงดู จะเห็นว่ามันมีหลายตาราง เพราะฉะนั้น เพื่อให้เราได้เห็นรูปแบบที่หลากหลาย แล้วก็ครอบคลุมข้อมูลทั้งหมดมากที่สุด ก็ควรจะหยิบ sample ออกมาจากหลาย ๆ ที่ กระจาย ๆ กันหน่อยนั่นเองครับ เช่น เอามาจากตาราง 1 บ้าง จากตาราง 8 บ้าง จากตารางสุดท้ายบ้าง หรือจากหัวตารางบ้าง จากท้ายตารางบ้าง ปน ๆ กันไป  &lt;strong&gt;โดยหลักสถิติแล้วยิ่งเราสุ่มมามากเท่าไหร่ เราก็จะยิ่งมั่นใจได้ว่าเราได้เห็น pattern ที่เป็นไปได้ทั้งหมดแล้วมากขึ้นนั่นเองครับ&lt;/strong&gt;  (จะเรียกว่าหลักสถิติหรือดวงก็ได้นะครับ 555)&lt;/p&gt;

&lt;p&gt;สำหรับท่านที่ยังไม่เข้าใจว่าจะเอา sample XPath ตรงนี้มาได้อย่างไร  &lt;strong&gt;กรุณาอ่าน  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-2-chrome-s-code-inspector-3ok6&#34; target=&#34;_blank&#34;&gt;part 2&lt;/a&gt;  ด่วน ๆ เลยครับ!&lt;/strong&gt;  หรือถ้ามีข้อสงสัย สามารถคอมเม้นถามด้านล่าง หรือในเพจ  &lt;a href=&#34;https://www.facebook.com/CopyPasteEng/&#34; target=&#34;_blank&#34;&gt;Copy Paste Engineer&lt;/a&gt;  ได้เลยนะครับ&lt;/p&gt;

&lt;p&gt;ด้านล่างนี้เป็นตัวอย่างที่ผม sample ออกมา&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; (1) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[3]/tbody/tr[1]/td[3]/b/a    (ตาราง 1 บรรทัด 1)
 (2) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[3]/tbody/tr[2]/td[3]/b/a    (ตาราง 1 บรรทัด 2)
 (3) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[3]/tbody/tr[10]/td[3]/b/a   (ตาราง 1 บรรทัด 10)
 (4) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[4]/tbody/tr[3]/td[3]/b/a    (ตาราง 2 บรรทัด 1)
 (5) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[4]/tbody/tr[30]/td[2]/b/a   (ตาราง 2 บรรทัด 28)
 (6) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[7]/tbody/tr[25]/td[2]/b/a/b (ตาราง 5 บรรทัด 23)
 (7) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[10]/tbody/tr[41]/td[2]/b/a  (ตาราง 8 บรรทัดสุดท้าย)
 (8) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[24]/tbody/tr[3]/td[3]/a/b   (ตาราง 22 บรรทัด 1)
 (9) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[76]/tbody/tr[3]/td[3]/b/a   (ตารางสุดท้าย บรรทัด 1)
(10) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[76]/tbody/tr[37]/td[2]/b/a  (ตารางสุดท้าย บรรทัดสุดท้าย)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;จะเห็นว่า XPath จริง ๆ ของข้อมูลเรา มันไม่ได้ตรงไปตรงมาเสมอไปนะครับ อย่างเช่นตัวเลขของ table และ tr ใน XPath ของแต่ละ sample นั้น ก็อาจจะไม่ได้สัมพันธ์กับเลขตารางหรือเลขบรรทัดของข้อมูลเสมอไป หรือบางทีก็มี element b บ้าง บางทีก็ไม่มี  &lt;strong&gt;ซึ่งในเหตุการณ์เช่นนี้สามารถพบเจอได้จริง ในเว็บทั่ว ๆ ไปนะครับ&lt;/strong&gt;  โดยอาจเกิดจาก developer ที่เขียนโค้ดไม่สัมพันธ์กันเอง หรืออย่างในกรณีของ Wikipedia นี้ คืออาจเกิดจาก user ซึ่งเป็นผู้เขียนบทความได้ใช้โครงสร้างของข้อมูลที่แตกต่างกันนั่นเองครับ&lt;/p&gt;

&lt;h3 id=&#34;2-สำรวจ-xpath&#34;&gt;2. สำรวจ XPath&lt;/h3&gt;

&lt;p&gt;ในขั้นตอนนี้ เราจะทำการวิเคราะห์ XPath ที่เรา sample ออกมา แล้วก็ตั้งข้อสังเกตุถึง  &lt;strong&gt;จุดที่เหมือน&lt;/strong&gt;  และ  &lt;strong&gt;จุดที่แตกต่าง&lt;/strong&gt;  สำหรับแต่ละตัวอย่างนะครับ ซึ่งอาจจะต้องใช้ความรู้ HTML เล็กน้อยนะครับ แต่จะพยายามอธิบายให้เข้าใจง่ายที่สุดนะครับ&lt;/p&gt;

&lt;p&gt;เมื่อเรา sample ออกมาดูเยอะ ๆ แล้วก็จะเห็นครับว่ามันมีส่วนที่เหมือนกันอยู่ คือ ขึ้นต้นด้วย element  &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ซึ่ง  &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt;  ก็หมายถึงตัวตารางแต่ละตารางนั่นเองครับ&lt;/p&gt;

&lt;p&gt;และจะสังเกตุได้จาก XPath ที่ได้มา ว่าข้อมูลที่เราต้องการ&lt;br /&gt;
&lt;strong&gt;1.&lt;/strong&gt;  จะเริ่มต้นที่  &lt;strong&gt;ตารางที่ 3&lt;/strong&gt;  (เพราะในตารางที่ 1 บรรทัดที่ 1 ใช้  &lt;code&gt;table[3]&lt;/code&gt;)&lt;br /&gt;
&lt;strong&gt;2.&lt;/strong&gt;  แล้วตามด้วย  &lt;code&gt;&amp;lt;tbody&amp;gt;&lt;/code&gt;&lt;br /&gt;
&lt;strong&gt;3.&lt;/strong&gt;  ตามด้วย  &lt;code&gt;&amp;lt;tr&amp;gt;&lt;/code&gt;  คือแต่ละ row ของ table นั้น ๆ&lt;br /&gt;
&lt;strong&gt;4.&lt;/strong&gt;  จากนั้นก็  &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt;  คือแต่ละ column ของ row นั้น ๆ ซึ่งอาจจะเป็น  &lt;strong&gt;column ที่ 2 หรือ column ที่ 3 ก็ได้&lt;/strong&gt;  (เช่น sample  &lt;code&gt;(4)&lt;/code&gt;  และ  &lt;code&gt;(5)&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(4) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[4]/tbody/tr[3]/td[3]/b/a    (ตาราง 2 บรรทัด 1)
(5) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[4]/tbody/tr[30]/td[2]/b/a   (ตาราง 2 บรรทัด 28)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt;  แล้วก็อาจจะมี หรือไม่มี  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  ก็ได้ (กรณีที่ไม่มี  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  ดู sample ที่  &lt;code&gt;(8)&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(8) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[24]/tbody/tr[3]/td[3]/a/b   (ตาราง 22 บรรทัด 1)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;6.&lt;/strong&gt;  ตามด้วย  &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt;  คือข้อความที่เป็น link&lt;br /&gt;
&lt;strong&gt;7.&lt;/strong&gt;  แล้วก็อาจจะมี หรือไม่มี  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  ก็ได้&lt;/p&gt;

&lt;p&gt;นี่ก็เป็นข้อสังเกตุทั้งหมดที่เราได้จาก sample ของเรานะครับ&lt;/p&gt;

&lt;h3 id=&#34;3-สร-าง-xpath-จร-ง-ๆ-ส-กท-555&#34;&gt;3. สร้าง XPath (จริง ๆ สักที 555)**&lt;/h3&gt;

&lt;p&gt;ต่อไป จากข้อสังเกตุทั้ง 7 ข้อ ด้านบน เราก็ได้ทราบแล้วว่า elements ที่เราต้องการ จะมี XPath ลักษณะอย่างไร  &lt;strong&gt;ทีนี้เรานำข้อสังเกตุทั้งหมดมารวมกัน เพื่อสร้าง XPath ขึ้นมา 1 อัน ที่สามารถครอบคลุม cases ที่กล่าวมาได้ทั้งหมด!&lt;/strong&gt;  โดยใช้การเขียน  &lt;strong&gt;condition&lt;/strong&gt;  มาเพื่อช่วยเลือก elements ที่ต้องการนะครับ อันดับแรกก็เริ่มจาก XPath ที่ทุก sample มีร่วมกันก่อน ก็คือ&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ทีนี้ ก็จะเริ่มใส่ condition ตามข้อสังเกตุของเราข้างต้นแล้วนะครับ&lt;br /&gt;
&lt;strong&gt;1.&lt;/strong&gt;  เราต้องการเลือกตารางตั้งแต่ตารางที่ 3 ขึ้นไปเท่านั้น วิธีการคือให้ใส่ condition เข้าไป ว่า  &lt;strong&gt;position ของ  &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt;  เนี่ย ต้องมากกว่าหรือเท่ากับ 3 ครับ&lt;/strong&gt;  ซึ่งเขียนเป็น condition ได้ว่า  &lt;code&gt;position() &amp;gt;= 3&lt;/code&gt;  ก็จะได้ XPath เป็น&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[position() &amp;gt;= 3]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt;  ตามด้วย  &lt;code&gt;&amp;lt;tbody&amp;gt;&lt;/code&gt;  และ  &lt;code&gt;&amp;lt;tr&amp;gt;&lt;/code&gt;  ก็เติมเข้าไปตรง ๆ ได้เลยครับ&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[position() &amp;gt;= 3]/tbody/tr

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt;  ตามด้วย  &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt;  แต่เราจะเอาเฉพาะ column ที่ 2 หรือ 3 เท่านั้น วิธีหนึ่งที่ทำได้ก็คือ ใส่ condition ว่า  &lt;strong&gt;position ของ  &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt;  เนี่ย ต้อง &amp;gt;= 2 และ &amp;lt;= 3&lt;/strong&gt;  ซึ่งเขียน condition ออกมาได้ว่า  &lt;code&gt;2 &amp;lt;= position() and position() &amp;lt;= 3&lt;/code&gt;  จึงได้ XPath เป็น&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-c.../tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt;  อย่างที่สังเกตุว่าบาง sample ก็มี  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  ขึ้นก่อนแล้วตามด้วย  &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt;  (&lt;code&gt;/b/a/&lt;/code&gt;)&lt;br /&gt;
หรือบาง sample ก็เป็น  &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt;  ขึ้นก่อน แล้วตามด้วย  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  (&lt;code&gt;/a/b/&lt;/code&gt;)&lt;br /&gt;
แล้วยังมี  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  แล้ว  &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt;  แล้ว  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  อีก (&lt;code&gt;/b/a/b/&lt;/code&gt;)&lt;br /&gt;
โชคดีที่ใน XPath เราสามารถละได้ โดยการเขียนเป็น  &lt;code&gt;//a//&lt;/code&gt;  จะหมายถึงว่าจะมี element อะไรขึ้นก่อน  &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt;  ก็ได้ (กี่ elements ก็ได้ หรือจะไม่มีก็ได้) แล้วตามด้วย elements อะไรก็ได้ (กี่ elements ก็ได้ หรือจะไม่มีก็ได้) แล้วตัว engine ก็จะหาทุก ๆ elements ที่เป็นไปได้มาให้เองครับ&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-c.../tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]//a//

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;note: ซึ่งการละใน XPath นี้ ต้องระวังในการใช้สักเล็กน้อยนะครับ อย่าง  &lt;code&gt;//a//&lt;/code&gt;  อาจหมายถึง  &lt;code&gt;/b/a&lt;/code&gt;  ก็ได้ หรือ  &lt;code&gt;/b/a/b&lt;/code&gt;  ก็ได้ หรืออาจหมายถึง  &lt;code&gt;/b/div/a/div/b/a/b/div&lt;/code&gt;  ก็ได้ คือในส่วนที่ละ จะแทนเป็นอะไรก็ได้นั่นเองครับ&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt;  สุดท้ายตามด้วย  &lt;code&gt;text()&lt;/code&gt;  คือต้องการเอาเฉพาะข้อความที่อยู่ใน element นั้นออกมา&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-c.../tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]//a//text()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ได้ XPath สุดท้ายเป็น&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[position() &amp;gt;= 3]/tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]//a//text()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ซึ่งเป็น XPath ที่เรามั่นใจว่าและครอบคลุมทุกกรณีที่เรา sample ออกมา และ(หวังว่า)จะครอบคลุมข้อมูลที่เราต้องการจริง ๆ ทั้งหมด&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;และเป็น XPath ที่เราก็ได้นำไปใช้จริงแล้วในการ extract  &lt;em&gt;รายชื่อเทศบาลตำบล&lt;/em&gt;  ใน  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-1-python-49ce&#34; target=&#34;_blank&#34;&gt;บทความ part ที่ 1&lt;/a&gt;  นั่นเองครับ&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;สำหรับการสร้าง XPath นั้น วิธีการที่นำเสนอไปก็เป็นหนึ่งในวิธีที่สามารถทำได้ง่าย ๆ ซึ่งเราจะสามารถมั่นใจได้ในระดับหนึ่งว่า XPath ที่ได้มานั้น  &lt;strong&gt;ครอบคลุมข้อมูลส่วนใหญ่ โดยทำการ sample ออกมาให้มากขึ้น และ sample  &lt;em&gt;ในหลาย ๆ จุดที่แตกต่างกัน กระจาย ๆ กันไป&lt;/em&gt;&lt;/strong&gt;  แต่อาจจะต้องอาศัยความรู้ในเรื่อง syntax และคำสั่งของ XPath สักเล็กน้อยสำหรับการเขียน condition เพื่อ select element ต่าง ๆ นะครับ&lt;/p&gt;

&lt;p&gt;ซึ่งจริง ๆ แล้ว  &lt;strong&gt;ผู้เขียนเองก็ไม่ได้จำคำสั่งของ XPath ได้ทั้งหมดจริง ๆ หรอกครับ&lt;/strong&gt;  เมื่อจำเป็นต้องใช้ที ก็ search หาจาก internet เอา โดยผู้เขียนพบว่า  &lt;strong&gt;เว็บ  &lt;em&gt;w3school&lt;/em&gt;  ได้ทำรายการของคำสั่งต่าง ๆ พร้อมคำอธิบายเอาไว้ได้ดีและครบถ้วนแล้ว ทุก ๆ ท่านสามารถนำไปใช้เพื่ออ้างอิงขณะที่เขียนได้เลยครับ  &lt;a href=&#34;https://www.w3schools.com/xml/xpath_intro.asp&#34; target=&#34;_blank&#34;&gt;link นี้เลยครับ&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;และถ้ามีเรื่องไหนที่สนใจเพิ่มเติมสามารถ comment เอาไว้ได้นะครับ&lt;/p&gt;

&lt;p&gt;FB Page:  &lt;a href=&#34;https://www.facebook.com/CopyPasteEng&#34; target=&#34;_blank&#34;&gt;Copy Paste Engineer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part อื่น ๆ ใน series&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-1/&#34;&gt; Part I - การดูดข้อมูลเบื้องต้น ด้วย Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-2/&#34;&gt;Part II - Chrome&amp;rsquo;s Code Inspector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-3/&#34;&gt;Part III - เทคนิคการ extract ข้อมูลด้วย XPath&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Reference : &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-3-extract-xpath-18h&#34; target=&#34;_blank&#34;&gt;https://dev.to/copypasteengineer/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Python Web Scraping Part III - เทคนิคการ extract ข้อมูลด้วย XPath</title>
      <link>https://dragon-library.github.io/post/python/example/web-scraping-series-1/python-web-scraping-part-3/</link>
      <pubDate>Sun, 12 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://dragon-library.github.io/post/python/example/web-scraping-series-1/python-web-scraping-part-3/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;หลังจากที่ใน  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-2-chrome-s-code-inspector-3ok6&#34; target=&#34;_blank&#34;&gt;part ที่แล้ว&lt;/a&gt;  เราได้เรียนรู้วิธีการแกะ XPath ของแต่ละ element ออกมาจาก web browser แบบง่าย ๆ กันแล้ว แต่ทีนี้ในกรณีที่ข้อมูลที่เราสนใจมีหลายตำแหน่งมาก  &lt;strong&gt;เราคงไม่อยากเสียเวลานั่ง extract แต่ละ element ด้วยมือทั้งหมดใช่ไหมครับ&lt;/strong&gt;  (เพราะถ้าจะทำด้วยมือทั้งหมด ก็ก๊อปตัวข้อมูลมาตรง ๆ เลยสะดีกว่า 555)&lt;/p&gt;

&lt;p&gt;ท่านที่พอจะมีประสบการณ์การทำเว็บมาบ้าง พอเห็น XPath สัก 1 หรือ 2 อัน ก็อาจจะเดาได้แล้ว ว่า XPath ที่เหมาะสม ที่น่าจะครอบคลุมทุก ๆ element นั้น ควรจะเป็นอะไร แต่ถึงจะเดาไม่ออกก็ไม่เป็นไรครับ เพราะอย่างบาง website ที่ซับซ้อนขึ้นหน่อย ต่อให้มีประสบการณ์ก็คงเดาไม่ออกในทีแรกเช่นกัน&lt;/p&gt;

&lt;p&gt;สำหรับในบทความนี้ก็จะสอนวิธีหนึ่งที่สามารถทำได้ เพื่อหา XPath ที่  &lt;strong&gt;&amp;ldquo;น่าจะ&amp;rdquo;&lt;/strong&gt;  ครอบคลุมข้อมูลทั้งหมดที่เราต้องการมานะครับ ซึ่งเป็นวิธีที่ผู้เขียนเองก็ใช้งานอยู่จริงในการ scrape website ในหลาย ๆ ครั้งครับ (ที่บอกว่า  &lt;strong&gt;&amp;ldquo;น่าจะ&amp;rdquo;&lt;/strong&gt;  เนี่ย เพราะว่าในความเป็นจริงไม่มีใครรู้หรอกครับ ว่า website เป้าหมายนี้ ถูกเขียน หรือถูกสร้างมาอย่างไร แต่ด้วยวิธีดังกล่าว+หลักของสถิติ เราก็จะมั่นใจได้ในระดับหนึ่งว่า XPath ของเรามัน  &lt;strong&gt;&amp;ldquo;น่าจะ&amp;rdquo;&lt;/strong&gt;  ดีเพียงพอครับ)&lt;/p&gt;

&lt;p&gt;วิธีหนึ่งที่ผมทำก็คือ sample XPath ออกมาดู เอามา  &lt;strong&gt;สำรวจ&lt;/strong&gt;  เพิ่มอีกเยอะ ๆ ก็จะช่วยให้เห็นภาพมากขึ้นครับ จากนั้นเราก็จะใช้ syntax ของ XPath ในการ select และระบุ condition เพื่อให้ครอบคลุมกรณีที่เรา sample ออกมา ให้มากที่สุดครับ&lt;/p&gt;

&lt;h2 id=&#34;get-started&#34;&gt;Get Started!&lt;/h2&gt;

&lt;p&gt;ทบทวนสักเล็กน้อย ใน part นี้เราก็ยังอยู่กับเว็บ Wikipedia  &lt;a href=&#34;https://th.wikipedia.org/wiki/%E0%B8%A3%E0%B8%B2%E0%B8%A2%E0%B8%8A%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B9%80%E0%B8%97%E0%B8%A8%E0%B8%9A%E0%B8%B2%E0%B8%A5%E0%B8%95%E0%B8%B3%E0%B8%9A%E0%B8%A5%E0%B9%83%E0%B8%99%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B9%80%E0%B8%97%E0%B8%A8%E0%B9%84%E0%B8%97%E0%B8%A2&#34; target=&#34;_blank&#34;&gt;https://th.wikipedia.org/wiki/รายชื่อเทศบาลตำบลในประเทศไทย&lt;/a&gt;  กันเหมือนเดิมนะครับ โดยเป้าหมายคือ  &lt;strong&gt;หา XPath สำหรับ extract รายชื่อเทศบาลตำบลทั้งหมด&lt;/strong&gt;  ซึ่งใน  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-2-chrome-s-code-inspector-3ok6&#34; target=&#34;_blank&#34;&gt;part ที่แล้ว&lt;/a&gt;  เราใช้ Chrome Inspector ในการแกะ XPath ของ  &lt;strong&gt;&amp;ldquo;Element&amp;rdquo; หนึ่ง ๆ&lt;/strong&gt;  มาได้แบบง่าย ๆ แล้วนะครับ ในตอนนี้เรากำลังจะหา XPath ที่สามารถ extract ข้อมูลทั้งหมดได้  &lt;strong&gt;(คือครอบคลุมทุก element ไม่ใช่แค่ 1 element)&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;1-sample-xpath-มาเพ-ม&#34;&gt;1. Sample XPath มาเพิ่ม&lt;/h3&gt;

&lt;p&gt;อย่างในเว็บเป้าหมาย ถ้าลอง scroll เลื่อนขึ้นลงดู จะเห็นว่ามันมีหลายตาราง เพราะฉะนั้น เพื่อให้เราได้เห็นรูปแบบที่หลากหลาย แล้วก็ครอบคลุมข้อมูลทั้งหมดมากที่สุด ก็ควรจะหยิบ sample ออกมาจากหลาย ๆ ที่ กระจาย ๆ กันหน่อยนั่นเองครับ เช่น เอามาจากตาราง 1 บ้าง จากตาราง 8 บ้าง จากตารางสุดท้ายบ้าง หรือจากหัวตารางบ้าง จากท้ายตารางบ้าง ปน ๆ กันไป  &lt;strong&gt;โดยหลักสถิติแล้วยิ่งเราสุ่มมามากเท่าไหร่ เราก็จะยิ่งมั่นใจได้ว่าเราได้เห็น pattern ที่เป็นไปได้ทั้งหมดแล้วมากขึ้นนั่นเองครับ&lt;/strong&gt;  (จะเรียกว่าหลักสถิติหรือดวงก็ได้นะครับ 555)&lt;/p&gt;

&lt;p&gt;สำหรับท่านที่ยังไม่เข้าใจว่าจะเอา sample XPath ตรงนี้มาได้อย่างไร  &lt;strong&gt;กรุณาอ่าน  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-2-chrome-s-code-inspector-3ok6&#34; target=&#34;_blank&#34;&gt;part 2&lt;/a&gt;  ด่วน ๆ เลยครับ!&lt;/strong&gt;  หรือถ้ามีข้อสงสัย สามารถคอมเม้นถามด้านล่าง หรือในเพจ  &lt;a href=&#34;https://www.facebook.com/CopyPasteEng/&#34; target=&#34;_blank&#34;&gt;Copy Paste Engineer&lt;/a&gt;  ได้เลยนะครับ&lt;/p&gt;

&lt;p&gt;ด้านล่างนี้เป็นตัวอย่างที่ผม sample ออกมา&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; (1) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[3]/tbody/tr[1]/td[3]/b/a    (ตาราง 1 บรรทัด 1)
 (2) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[3]/tbody/tr[2]/td[3]/b/a    (ตาราง 1 บรรทัด 2)
 (3) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[3]/tbody/tr[10]/td[3]/b/a   (ตาราง 1 บรรทัด 10)
 (4) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[4]/tbody/tr[3]/td[3]/b/a    (ตาราง 2 บรรทัด 1)
 (5) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[4]/tbody/tr[30]/td[2]/b/a   (ตาราง 2 บรรทัด 28)
 (6) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[7]/tbody/tr[25]/td[2]/b/a/b (ตาราง 5 บรรทัด 23)
 (7) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[10]/tbody/tr[41]/td[2]/b/a  (ตาราง 8 บรรทัดสุดท้าย)
 (8) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[24]/tbody/tr[3]/td[3]/a/b   (ตาราง 22 บรรทัด 1)
 (9) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[76]/tbody/tr[3]/td[3]/b/a   (ตารางสุดท้าย บรรทัด 1)
(10) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[76]/tbody/tr[37]/td[2]/b/a  (ตารางสุดท้าย บรรทัดสุดท้าย)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;จะเห็นว่า XPath จริง ๆ ของข้อมูลเรา มันไม่ได้ตรงไปตรงมาเสมอไปนะครับ อย่างเช่นตัวเลขของ table และ tr ใน XPath ของแต่ละ sample นั้น ก็อาจจะไม่ได้สัมพันธ์กับเลขตารางหรือเลขบรรทัดของข้อมูลเสมอไป หรือบางทีก็มี element b บ้าง บางทีก็ไม่มี  &lt;strong&gt;ซึ่งในเหตุการณ์เช่นนี้สามารถพบเจอได้จริง ในเว็บทั่ว ๆ ไปนะครับ&lt;/strong&gt;  โดยอาจเกิดจาก developer ที่เขียนโค้ดไม่สัมพันธ์กันเอง หรืออย่างในกรณีของ Wikipedia นี้ คืออาจเกิดจาก user ซึ่งเป็นผู้เขียนบทความได้ใช้โครงสร้างของข้อมูลที่แตกต่างกันนั่นเองครับ&lt;/p&gt;

&lt;h3 id=&#34;2-สำรวจ-xpath&#34;&gt;2. สำรวจ XPath&lt;/h3&gt;

&lt;p&gt;ในขั้นตอนนี้ เราจะทำการวิเคราะห์ XPath ที่เรา sample ออกมา แล้วก็ตั้งข้อสังเกตุถึง  &lt;strong&gt;จุดที่เหมือน&lt;/strong&gt;  และ  &lt;strong&gt;จุดที่แตกต่าง&lt;/strong&gt;  สำหรับแต่ละตัวอย่างนะครับ ซึ่งอาจจะต้องใช้ความรู้ HTML เล็กน้อยนะครับ แต่จะพยายามอธิบายให้เข้าใจง่ายที่สุดนะครับ&lt;/p&gt;

&lt;p&gt;เมื่อเรา sample ออกมาดูเยอะ ๆ แล้วก็จะเห็นครับว่ามันมีส่วนที่เหมือนกันอยู่ คือ ขึ้นต้นด้วย element  &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ซึ่ง  &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt;  ก็หมายถึงตัวตารางแต่ละตารางนั่นเองครับ&lt;/p&gt;

&lt;p&gt;และจะสังเกตุได้จาก XPath ที่ได้มา ว่าข้อมูลที่เราต้องการ&lt;br /&gt;
&lt;strong&gt;1.&lt;/strong&gt;  จะเริ่มต้นที่  &lt;strong&gt;ตารางที่ 3&lt;/strong&gt;  (เพราะในตารางที่ 1 บรรทัดที่ 1 ใช้  &lt;code&gt;table[3]&lt;/code&gt;)&lt;br /&gt;
&lt;strong&gt;2.&lt;/strong&gt;  แล้วตามด้วย  &lt;code&gt;&amp;lt;tbody&amp;gt;&lt;/code&gt;&lt;br /&gt;
&lt;strong&gt;3.&lt;/strong&gt;  ตามด้วย  &lt;code&gt;&amp;lt;tr&amp;gt;&lt;/code&gt;  คือแต่ละ row ของ table นั้น ๆ&lt;br /&gt;
&lt;strong&gt;4.&lt;/strong&gt;  จากนั้นก็  &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt;  คือแต่ละ column ของ row นั้น ๆ ซึ่งอาจจะเป็น  &lt;strong&gt;column ที่ 2 หรือ column ที่ 3 ก็ได้&lt;/strong&gt;  (เช่น sample  &lt;code&gt;(4)&lt;/code&gt;  และ  &lt;code&gt;(5)&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(4) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[4]/tbody/tr[3]/td[3]/b/a    (ตาราง 2 บรรทัด 1)
(5) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[4]/tbody/tr[30]/td[2]/b/a   (ตาราง 2 บรรทัด 28)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt;  แล้วก็อาจจะมี หรือไม่มี  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  ก็ได้ (กรณีที่ไม่มี  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  ดู sample ที่  &lt;code&gt;(8)&lt;/code&gt;)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(8) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[24]/tbody/tr[3]/td[3]/a/b   (ตาราง 22 บรรทัด 1)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;6.&lt;/strong&gt;  ตามด้วย  &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt;  คือข้อความที่เป็น link&lt;br /&gt;
&lt;strong&gt;7.&lt;/strong&gt;  แล้วก็อาจจะมี หรือไม่มี  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  ก็ได้&lt;/p&gt;

&lt;p&gt;นี่ก็เป็นข้อสังเกตุทั้งหมดที่เราได้จาก sample ของเรานะครับ&lt;/p&gt;

&lt;h3 id=&#34;3-สร-าง-xpath-จร-ง-ๆ-ส-กท-555&#34;&gt;3. สร้าง XPath (จริง ๆ สักที 555)**&lt;/h3&gt;

&lt;p&gt;ต่อไป จากข้อสังเกตุทั้ง 7 ข้อ ด้านบน เราก็ได้ทราบแล้วว่า elements ที่เราต้องการ จะมี XPath ลักษณะอย่างไร  &lt;strong&gt;ทีนี้เรานำข้อสังเกตุทั้งหมดมารวมกัน เพื่อสร้าง XPath ขึ้นมา 1 อัน ที่สามารถครอบคลุม cases ที่กล่าวมาได้ทั้งหมด!&lt;/strong&gt;  โดยใช้การเขียน  &lt;strong&gt;condition&lt;/strong&gt;  มาเพื่อช่วยเลือก elements ที่ต้องการนะครับ อันดับแรกก็เริ่มจาก XPath ที่ทุก sample มีร่วมกันก่อน ก็คือ&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ทีนี้ ก็จะเริ่มใส่ condition ตามข้อสังเกตุของเราข้างต้นแล้วนะครับ&lt;br /&gt;
&lt;strong&gt;1.&lt;/strong&gt;  เราต้องการเลือกตารางตั้งแต่ตารางที่ 3 ขึ้นไปเท่านั้น วิธีการคือให้ใส่ condition เข้าไป ว่า  &lt;strong&gt;position ของ  &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt;  เนี่ย ต้องมากกว่าหรือเท่ากับ 3 ครับ&lt;/strong&gt;  ซึ่งเขียนเป็น condition ได้ว่า  &lt;code&gt;position() &amp;gt;= 3&lt;/code&gt;  ก็จะได้ XPath เป็น&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[position() &amp;gt;= 3]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt;  ตามด้วย  &lt;code&gt;&amp;lt;tbody&amp;gt;&lt;/code&gt;  และ  &lt;code&gt;&amp;lt;tr&amp;gt;&lt;/code&gt;  ก็เติมเข้าไปตรง ๆ ได้เลยครับ&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[position() &amp;gt;= 3]/tbody/tr

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt;  ตามด้วย  &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt;  แต่เราจะเอาเฉพาะ column ที่ 2 หรือ 3 เท่านั้น วิธีหนึ่งที่ทำได้ก็คือ ใส่ condition ว่า  &lt;strong&gt;position ของ  &lt;code&gt;&amp;lt;td&amp;gt;&lt;/code&gt;  เนี่ย ต้อง &amp;gt;= 2 และ &amp;lt;= 3&lt;/strong&gt;  ซึ่งเขียน condition ออกมาได้ว่า  &lt;code&gt;2 &amp;lt;= position() and position() &amp;lt;= 3&lt;/code&gt;  จึงได้ XPath เป็น&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-c.../tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;4.&lt;/strong&gt;  อย่างที่สังเกตุว่าบาง sample ก็มี  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  ขึ้นก่อนแล้วตามด้วย  &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt;  (&lt;code&gt;/b/a/&lt;/code&gt;)&lt;br /&gt;
หรือบาง sample ก็เป็น  &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt;  ขึ้นก่อน แล้วตามด้วย  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  (&lt;code&gt;/a/b/&lt;/code&gt;)&lt;br /&gt;
แล้วยังมี  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  แล้ว  &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt;  แล้ว  &lt;code&gt;&amp;lt;b&amp;gt;&lt;/code&gt;  อีก (&lt;code&gt;/b/a/b/&lt;/code&gt;)&lt;br /&gt;
โชคดีที่ใน XPath เราสามารถละได้ โดยการเขียนเป็น  &lt;code&gt;//a//&lt;/code&gt;  จะหมายถึงว่าจะมี element อะไรขึ้นก่อน  &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt;  ก็ได้ (กี่ elements ก็ได้ หรือจะไม่มีก็ได้) แล้วตามด้วย elements อะไรก็ได้ (กี่ elements ก็ได้ หรือจะไม่มีก็ได้) แล้วตัว engine ก็จะหาทุก ๆ elements ที่เป็นไปได้มาให้เองครับ&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-c.../tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]//a//

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;note: ซึ่งการละใน XPath นี้ ต้องระวังในการใช้สักเล็กน้อยนะครับ อย่าง  &lt;code&gt;//a//&lt;/code&gt;  อาจหมายถึง  &lt;code&gt;/b/a&lt;/code&gt;  ก็ได้ หรือ  &lt;code&gt;/b/a/b&lt;/code&gt;  ก็ได้ หรืออาจหมายถึง  &lt;code&gt;/b/div/a/div/b/a/b/div&lt;/code&gt;  ก็ได้ คือในส่วนที่ละ จะแทนเป็นอะไรก็ได้นั่นเองครับ&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5.&lt;/strong&gt;  สุดท้ายตามด้วย  &lt;code&gt;text()&lt;/code&gt;  คือต้องการเอาเฉพาะข้อความที่อยู่ใน element นั้นออกมา&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-c.../tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]//a//text()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ได้ XPath สุดท้ายเป็น&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[position() &amp;gt;= 3]/tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]//a//text()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ซึ่งเป็น XPath ที่เรามั่นใจว่าและครอบคลุมทุกกรณีที่เรา sample ออกมา และ(หวังว่า)จะครอบคลุมข้อมูลที่เราต้องการจริง ๆ ทั้งหมด&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;และเป็น XPath ที่เราก็ได้นำไปใช้จริงแล้วในการ extract  &lt;em&gt;รายชื่อเทศบาลตำบล&lt;/em&gt;  ใน  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-1-python-49ce&#34; target=&#34;_blank&#34;&gt;บทความ part ที่ 1&lt;/a&gt;  นั่นเองครับ&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;สำหรับการสร้าง XPath นั้น วิธีการที่นำเสนอไปก็เป็นหนึ่งในวิธีที่สามารถทำได้ง่าย ๆ ซึ่งเราจะสามารถมั่นใจได้ในระดับหนึ่งว่า XPath ที่ได้มานั้น  &lt;strong&gt;ครอบคลุมข้อมูลส่วนใหญ่ โดยทำการ sample ออกมาให้มากขึ้น และ sample  &lt;em&gt;ในหลาย ๆ จุดที่แตกต่างกัน กระจาย ๆ กันไป&lt;/em&gt;&lt;/strong&gt;  แต่อาจจะต้องอาศัยความรู้ในเรื่อง syntax และคำสั่งของ XPath สักเล็กน้อยสำหรับการเขียน condition เพื่อ select element ต่าง ๆ นะครับ&lt;/p&gt;

&lt;p&gt;ซึ่งจริง ๆ แล้ว  &lt;strong&gt;ผู้เขียนเองก็ไม่ได้จำคำสั่งของ XPath ได้ทั้งหมดจริง ๆ หรอกครับ&lt;/strong&gt;  เมื่อจำเป็นต้องใช้ที ก็ search หาจาก internet เอา โดยผู้เขียนพบว่า  &lt;strong&gt;เว็บ  &lt;em&gt;w3school&lt;/em&gt;  ได้ทำรายการของคำสั่งต่าง ๆ พร้อมคำอธิบายเอาไว้ได้ดีและครบถ้วนแล้ว ทุก ๆ ท่านสามารถนำไปใช้เพื่ออ้างอิงขณะที่เขียนได้เลยครับ  &lt;a href=&#34;https://www.w3schools.com/xml/xpath_intro.asp&#34; target=&#34;_blank&#34;&gt;link นี้เลยครับ&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;และถ้ามีเรื่องไหนที่สนใจเพิ่มเติมสามารถ comment เอาไว้ได้นะครับ&lt;/p&gt;

&lt;p&gt;FB Page:  &lt;a href=&#34;https://www.facebook.com/CopyPasteEng&#34; target=&#34;_blank&#34;&gt;Copy Paste Engineer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part อื่น ๆ ใน series&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-1/&#34;&gt; Part I - การดูดข้อมูลเบื้องต้น ด้วย Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-2/&#34;&gt;Part II - Chrome&amp;rsquo;s Code Inspector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-4/&#34;&gt;Part IV - ทำไมถึง scrape บางเว็บไม่ได้&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Reference : &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-3-extract-xpath-18h&#34; target=&#34;_blank&#34;&gt;https://dev.to/copypasteengineer/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Python Web Scraping Part II - Chrome&#39;s Code Inspector</title>
      <link>https://dragon-library.github.io/post/python/example/web-scraping-series-1/python-web-scraping-part-2/</link>
      <pubDate>Sun, 05 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://dragon-library.github.io/post/python/example/web-scraping-series-1/python-web-scraping-part-2/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;เราได้ลองดึงข้อมูลจาก Wikipedia ด้วย Python แบบง่าย ๆ ดูแล้ว ถ้ายังจำได้ หลังจากที่เรา  &lt;strong&gt;scrape&lt;/strong&gt;  &lt;strong&gt;ข้อมูลดิบ&lt;/strong&gt;  เป็นโค้ด HTML ยาว ๆ ลงมาใน Python แล้ว ในส่วนของการ  &lt;strong&gt;extract ข้อมูล&lt;/strong&gt;  เราใช้  &lt;strong&gt;XPath&lt;/strong&gt;  เพื่อ select elements ที่ต้องการออกมา&lt;/p&gt;

&lt;p&gt;ซึ่งผู้เขียนได้แอบบอกไว้แล้วว่า XPath ที่ได้มานั้น มาจากการแกะโค้ด HTML ของเว็บต้นทาง โดยใช้เครื่องมือ  &lt;strong&gt;Code Inspector&lt;/strong&gt;  บน  &lt;strong&gt;Google Chrome Browser&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;บทความนี้จะมาพูดถึงเครื่องมือตัวนี้กันครับ ว่ามันจะช่วยให้เราดูดข้อมูลได้ง่ายขึ้นอย่างไรบ้าง&lt;/p&gt;

&lt;p&gt;จริง ๆ แล้วตัว  &lt;strong&gt;Code Inspector&lt;/strong&gt;  นี่มันเป็นแค่หนึ่งในหลาย ๆ tools ที่  &lt;strong&gt;Google Chrome&lt;/strong&gt;  มีให้ใน  &lt;strong&gt;Google Chrome DevTools&lt;/strong&gt;  เท่านั้นนะครับ โดยจุดประสงค์หลัก ๆ ของ tools ทั้งหลายก็คือใช้เพื่อ debug เว็บที่เราเขียนขึ้นเอง โดยผู้ใช้สามารถเรียกดูได้ทั้งโครงสร้าง การทำงาน และ data ของเว็บที่รันอยู่แบบ real-time เลย  &lt;strong&gt;ซึ่งล้วนแล้วแต่เป็น information ที่มีประโยชน์ต่อการ scrape ข้อมูลทั้งสิ้นครับ&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;โดยในครั้งนี้จะมาแนะนำ  &lt;strong&gt;Code Inspector&lt;/strong&gt;  ให้รู้จักกันคร่าว ๆ ว่ามันทำอะไรได้บ้าง แล้วจะใช้เพื่อแกะ XPath ออกมาจากโค้ดได้อย่างไรนะครับ แล้วในบทความถัด ๆ ไป จะสอน tool อีกตัวที่ช่วยให้เราเข้าใจการทำงานของเว็บนั้น ๆ มากขึ้น เช่น กดปุ่มนี้แล้วเว็บจะโหลดข้อมูลอะไรมาจากที่ไหน ด้วย tool ที่ชื่อว่า  &lt;strong&gt;Network Inspector&lt;/strong&gt;  บน Google Chrome เช่นกันครับ&lt;/p&gt;

&lt;h2 id=&#34;get-started&#34;&gt;Get Started!&lt;/h2&gt;

&lt;p&gt;ตัว Code Inspector นี่ สำหรับ developer จะเอาไว้ใช้ตรวจสอบว่าหน้าเว็บที่เราเขียนนั้น render ออกมาเป็น code HTML ได้ถูกต้องหรือไม่ ทั้งโครงสร้างของ HTML และ attributes ต่าง ๆ&lt;/p&gt;

&lt;p&gt;&lt;em&gt;note: อธิบายสำหรับท่านที่ไม่ได้เป็น web developer นะครับ บางที web page ที่เราเห็นนั้นอาจจะไม่ได้เขียนด้วย HTML ไฟล์เดียวทั้งหมดนะครับ อาจจะมาจาก HTML หลายไฟล์มารวมกัน หรือถูก render ลงมาเป็น HTML จาก server ต้นทางอีกที ทำให้ในการ develop ฝั่ง server อาจจะต้องดูผลลัพธ์ไปพร้อม ๆ กัน ว่า HTML code ถูก rendered ออกมาตามที่ต้องการหรือไม่&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;ซึ่งประโยชน์หลัก ๆ ที่เราจะได้จาก Code Inspector ก็คือใช้หา XPath หรือหาวิธีการ extract ข้อมูลจากหน้าเว็บนั้น ๆ เช่น ดู id ของ elements ที่เราสนใจ&lt;/p&gt;

&lt;p&gt;วิธีการเรียก Code Inspect คือให้เข้าไปที่ website ที่ต้องการ ด้วย  &lt;strong&gt;Google Chrome Browser&lt;/strong&gt;  แล้วกด  &lt;code&gt;F12&lt;/code&gt;  ก็จะมีแถบด้านขวา (หรือด้านล่าง) ที่แสดง HTML elements ของ page ปัจจุบัน ปรากฎออกมาตามภาพ&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--aIFdosyu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/sdawbnm8vkmgun33g31u.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--aIFdosyu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/sdawbnm8vkmgun33g31u.png&#34; alt=&#34;code-inspector-1.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;highlight-ตำแหน-งของ-component-จาก-element&#34;&gt;Highlight ตำแหน่งของ Component จาก Element&lt;/h3&gt;

&lt;p&gt;โดยเราสามารถ  &lt;strong&gt;คลิกเพื่อ expand elements ย่อย ๆ ออก&lt;/strong&gt;  แล้วเข้าไปดูโครงสร้างที่ซ่อนไว้ได้ หรือถ้าหากลองเอาเมาส์ไปชี้ที่ element ในนั้นดู Chrome ก็จะข่วย highlight บอกเราให้ด้วย ว่า element นั้น อยู่ตรงไหนบนหน้าเว็บ&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--E1mB_gkv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/p7ap5s21e969faqgko4l.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--E1mB_gkv--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/p7ap5s21e969faqgko4l.png&#34; alt=&#34;code-inspector-highlight.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;highlight-element-จาก-component&#34;&gt;Highlight Element จาก Component&lt;/h3&gt;

&lt;p&gt;หรือในกรณีที่เราอยากทราบว่าตัว component บนหน้าเว็บ เช่น ตารางที่เราเห็นอยู่นี้ มันคือ element ในโค้ด HTML ก็สามารถทำได้ โดยการ  &lt;strong&gt;คลิกขวาที่ component นั้น ๆ แล้วเลือก Inspect&lt;/strong&gt;  จากนั้นตัว Code Inspector ก็จะไป highlight HTML element ของ component ให้ทันทีครับ&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--tT7MQ41b--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/9qgsvj4nwwmso24tjlp9.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--tT7MQ41b--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/9qgsvj4nwwmso24tjlp9.png&#34; alt=&#34;code-inspector-find-element.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;แกะ-xpath-ของ-component-แบบเบส-ก&#34;&gt;แกะ XPath ของ Component แบบเบสิก&lt;/h3&gt;

&lt;p&gt;เมื่อเราคลิกเลือก element ที่เราสนใจได้แล้ว หากเราอยากจะได้ XPath ของ element นั้น ๆ มา Inspector ก็สามารถหาให้เราแบบง่าย ๆ ได้ (แต่อาจจะไม่ค่อยตรงใจเราสักเท่าไหร่555)&lt;/p&gt;

&lt;p&gt;เช่น สมมุติว่าเราอยากจะได้ XPath ของ  &lt;code&gt;เทศบาลตำบลบ้านดู่&lt;/code&gt;  ก็ให้หา element ของมันให้ได้ก่อนตามที่อธิบายไว้ด้านบน คือคลิกขวาที่  &lt;code&gt;เทศบาลตำบลบ้านดู่&lt;/code&gt;  แล้วเลือก  &lt;code&gt;Inspect&lt;/code&gt;  Inspector ก็จะ highlight element ของ  &lt;code&gt;เทศบาลตำบลบ้านดู่&lt;/code&gt;  ไว้ให้&lt;br /&gt;
จากนั้นก็หา XPath โดยการ  &lt;strong&gt;คลิกขวาที่ element นั้น แล้วเลือก  &lt;code&gt;Copy&lt;/code&gt;  แล้วก็  &lt;code&gt;Copy XPath&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--lfYxlBnA--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/m6ht57jk45ms9x0w6owv.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--lfYxlBnA--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/m6ht57jk45ms9x0w6owv.png&#34; alt=&#34;code-inspector-copy-xpath.png&#34; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ถ้าลอง paste ดูก็จะได้ XPath ของ  &lt;code&gt;เทศบาลตำบลบ้านดู่&lt;/code&gt;  แบบนี้ครับ&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[3]/tbody/tr[1]/td[3]/b/a

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;แต่ XPath นี้เป็นแค่ของ  &lt;code&gt;เทศบาลตำบลบ้านดู่&lt;/code&gt;  เท่านั้นครับ&lt;/strong&gt;  แต่สิ่งที่ต้องการจริง ๆ คือ  &lt;strong&gt;ชื่อเทศบาลทั้งหมด&lt;/strong&gt;  ถามว่าจะหามาได้อย่างไร?&lt;/p&gt;

&lt;p&gt;ถ้ามีประสบการณ์การทำเว็บมาบ้าง พอเห็นข้างบนก็อาจจะเดาได้แล้วว่า XPath ที่เหมาะสมควรจะเป็นอะไรนะครับ แต่ถ้ายังเดาไม่ออกก็ไม่เป็นไรครับ เพราะอย่างบาง website ที่ซับซ้อนขึ้นหน่อย ถึงจะพอมีประสบการณ์ก็คงเดาไม่ออกในทีแรกเช่นกัน&lt;/p&gt;

&lt;p&gt;วิธีหนึ่งที่ผมทำก็คือใช้ตัว Chrome Inspector นี่แหละ เอาตัวอย่างของ XPath จากเทศบาลตำบลอันอื่น ๆ ออกมาดูเพิ่มอีกเยอะ ๆ ก็จะช่วยให้เห็นภาพมากขึ้นครับ&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; (1) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[3]/tbody/tr[1]/td[3]/b/a    (ตาราง 1 บรรทัด 1)
 (2) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[3]/tbody/tr[2]/td[3]/b/a    (ตาราง 1 บรรทัด 2)
 (3) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[3]/tbody/tr[10]/td[3]/b/a   (ตาราง 1 บรรทัด 10)
 (4) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[4]/tbody/tr[3]/td[3]/b/a    (ตาราง 2 บรรทัด 1)
 (5) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[4]/tbody/tr[30]/td[2]/b/a   (ตาราง 2 บรรทัด 28)
 (6) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[7]/tbody/tr[25]/td[2]/b/a/b (ตาราง 5 บรรทัด 23)
 (7) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[10]/tbody/tr[41]/td[2]/b/a  (ตาราง 8 บรรทัดสุดท้าย)
 (8) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[24]/tbody/tr[3]/td[3]/a/b   (ตาราง 22 บรรทัด 1)
 (9) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[76]/tbody/tr[3]/td[3]/b/a   (ตารางสุดท้าย บรรทัด 1)
(10) //*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[76]/tbody/tr[37]/td[2]/b/a  (ตารางสุดท้าย บรรทัดสุดท้าย)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;&amp;hellip;แล้วยังไงต่อ&amp;hellip;&lt;/strong&gt;  ใช่ไหมครับ 555&lt;br /&gt;
ต่อไปเราก็จะสร้าง XPath 1 อัน ที่สามารถครอบคลุมทุกกรณีที่เราเจอมาโดยการใส่ condition เพื่อช่วยเลือก โดยผลลัพธ์สุดท้ายก็จะได้เป็น XPath ที่เราใช้จริง ใน  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-1-python-49ce&#34; target=&#34;_blank&#34;&gt;part ที่ 1&lt;/a&gt;  นั่นเองครับ ซึ่งจะอธิบายวิธีการโดยละเอียดต่อใน part 3 นะครับ&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;สำหรับ part ที่ 2 นี้ จุดประสงค์หลัก ก็คือแนะนำ tool ที่ชื่อ  &lt;strong&gt;Code Inspector&lt;/strong&gt;  ใน  &lt;strong&gt;Google Chrome&lt;/strong&gt;  ครับ ซึ่งเป็น tool ที่ช่วยให้สามารถ extract ข้อมูลได้ง่ายขึ้นมาก และผู้เขียนได้ใช้หา XPath ไปจริง ๆ ใน  &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-1-python-49ce&#34; target=&#34;_blank&#34;&gt;บทความ part 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part อื่น ๆ ใน series&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-1/&#34;&gt; Part I - การดูดข้อมูลเบื้องต้น ด้วย Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-3/&#34;&gt;Part III - เทคนิคการ extract ข้อมูลด้วย XPath&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-4/&#34;&gt;Part IV - ทำไมถึง scrape บางเว็บไม่ได้&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Reference : &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-2-chrome-s-code-inspector-3ok6&#34; target=&#34;_blank&#34;&gt;https://dev.to/copypasteengineer/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Python Web Scraping Part I</title>
      <link>https://dragon-library.github.io/post/python/example/web-scraping-series-1/python-web-scraping-part-1/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://dragon-library.github.io/post/python/example/web-scraping-series-1/python-web-scraping-part-1/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;featured.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;การทำ  &lt;strong&gt;Web Scraping&lt;/strong&gt;  เพื่อเก็บข้อมูลที่มีอยู่ใน Internet มาทำการวิเคราะห์ เพราะสำหรับการวิเคราะห์ข้อมูล ซึ่งเป็นงานของ Data Scientist แล้ว สิ่งที่สำคัญที่สุดก็คงจะเป็น  &lt;em&gt;ข้อมูล&lt;/em&gt;  นั่นแหละครับ&lt;/p&gt;

&lt;p&gt;นอกจากนี้ บริษัทใหญ่ ๆ หลาย ๆ แห่ง ก็ยังใช้ประโยชน์จากการทำ Web Scraping ในการเก็บรวบรวมความคิดเห็นของ social เพื่อเอามาวิเคราะห์ปรับปรุงผลิตภัณฑ์ของตัวเองได้อีกด้วย เช่น เก็บ complains ต่าง ๆ เกี่ยวกับผลิตภัณฑ์ในแต่ละชนิด หรือแม้กระทั่งการทำ sentiment analysis ด้วย technique NLP เพื่อดูว่า  &lt;em&gt;ความรู้สึก&lt;/em&gt;  ของลูกค้าหลังการใช้บริการเป็นอย่างไร จาก comment หรือ post ต่าง ๆ ในโลกออนไลน์แบบ real-time&lt;/p&gt;

&lt;p&gt;ในบทความนี้ก็เลยอยากจะเล่า flow ของการดูดข้อมูลแบบง่าย ๆ ซึ่งเหมาะสำหรับท่านที่พอจะเขียน Python ได้บ้างจนถึงชำนาญ และสนใจศึกษาเกี่ยวกับการทำ Web Scraping ครับ โดยในตัวอย่างนี้จะใช้เว็บ Wikipedia ซึ่งสามารถ scrape ได้ง่าย เหมาะกับผู้เริ่มต้นครับ&lt;/p&gt;

&lt;p&gt;โดยในบทความนี้จะนำเสนอเพียงแค่ basic เพื่อให้ผู้อ่านได้เห็นภาพรวมกว้าง ๆ ก่อน และจะกลับมาอธิบายลงลึกในแต่ละส่วนแยกกันไปในบทความถัด ๆ ไปอีกทีครับ&lt;/p&gt;

&lt;h2 id=&#34;get-started&#34;&gt;&lt;strong&gt;Get Started!&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;ก่อนอื่นจะขอแนะนำเครื่องมือที่ต้องใช้ก่อน จากนั้นก็จะแนะนำ flow ของการ scrape แบบง่าย ๆ พร้อมกับตัวอย่างโค้ดให้นำไปทดลองกันได้ง่าย ๆ ครับ&lt;/p&gt;

&lt;h2 id=&#34;ส-งท-ต-องเตร-ยม&#34;&gt;&lt;strong&gt;สึ่งที่ต้องเตรียม&amp;hellip;&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1. Python&lt;/strong&gt;  แน่นอนว่าถ้าจะเขียน Python script ก็ต้องมีโปรแกรมสำหรับ run script ก่อน ซึ่งตรงนี้สำหรับท่านที่ยังไม่มี Python ในเครื่อง ก็สามารถ install ได้จาก  &lt;a href=&#34;https://www.python.org/downloads/&#34; target=&#34;_blank&#34;&gt;link นี้&lt;/a&gt;  โดยแนะนำให้ใช้ version ตั้งแต่  &lt;code&gt;3.6&lt;/code&gt;  ขึ้นไปครับ&lt;/p&gt;

&lt;p&gt;แต่ถ้าหากไม่สะดวก install ลงในเครื่องที่ใช้อยู่ก็สามารถ execute Python script online ได้โดยใช้บริการ  &lt;strong&gt;Google Colab Notebook&lt;/strong&gt;  ของ Google ซึ่งสามารถแก้ไข และ execute Python script ทีละ block ได้ ผ่าน web browser จึงไม่จำเป็นต้อง install หรือ download อะไรลงในเครื่องทั้งสิ้นครับ&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;โดยโค้ดทั้งหมดที่ใช้ในบทความนี้จะแชร์ผ่าน Google Colab ด้วยเช่นกัน เนื่องจาก Data Scientist ส่วนใหญ่น่าจะคุ้นเคยกับ interface ลักษณะนี้ดี&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--KMRZMmU1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/1y2k0fb0qwubrbnfbj1u.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--KMRZMmU1--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/1y2k0fb0qwubrbnfbj1u.png&#34; alt=&#34;google-colab.png&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;em&gt;ภาพ Google Colab Notebook&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. modules สำหรับการ scrape&lt;/strong&gt;  Python modules ที่จำเป็นต้องใช้ในบทความนี้ ได้แก่  &lt;code&gt;requests&lt;/code&gt;, และ  &lt;code&gt;lxml&lt;/code&gt;  โดยสามารถ install ผ่านโปรแกรม  &lt;code&gt;pip&lt;/code&gt;  ของ Python ได้จากคำสั่งต่อไปนี้&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;pip install requests lxml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;สำหรับท่านที่ใช้โค้ดบน Google Colab จะมี modules เหล่านี้ installed ไว้อยู่แล้ว ดังนั้นจึงไม่ต้องทำอะไรเพิ่มเติมครับ&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;เร-มก-นเถอะ&#34;&gt;&lt;strong&gt;เริ่มกันเถอะ!&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;code สำหรับบทความนี้จะแชร์ไว้ใน Google Colab ตาม  &lt;a href=&#34;https://colab.research.google.com/drive/1CxGihPm5if8YGOEjrVo9havGikqqQa6e&#34; target=&#34;_blank&#34;&gt;link นี้&lt;/a&gt;  นะครับ&lt;br /&gt;
ถ้าสะดวกสามารถเปิดโค้ดตามและทดลองแก้ไขและรันดูไปพร้อม ๆ กับที่อ่านบทความได้เลยนะครับ&lt;br /&gt;
โดยการคลิกแทบ  &lt;code&gt;File&lt;/code&gt;  ด้านบนของ Colab แล้วเลือก  &lt;code&gt;Save a copy in Drive...&lt;/code&gt;  ก็จะสามารถแก้ไขได้ และ save ลง Google Drive ให้อัตโนมัติครับ&lt;br /&gt;
ผู้อ่านสามารถรันโค้ดบน Colab ได้โดยการคลิกเลือกแต่ละ block แล้วกด  &lt;code&gt;Ctrl+Enter&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&#34;python-web-scraping&#34;&gt;&lt;strong&gt;Python Web Scraping&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;สมมุติว่า เราต้องการจะดูดข้อมูลจาก  &lt;a href=&#34;https://th.wikipedia.org/wiki/%E0%B8%A3%E0%B8%B2%E0%B8%A2%E0%B8%8A%E0%B8%B7%E0%B9%88%E0%B8%AD%E0%B9%80%E0%B8%97%E0%B8%A8%E0%B8%9A%E0%B8%B2%E0%B8%A5%E0%B8%95%E0%B8%B3%E0%B8%9A%E0%B8%A5%E0%B9%83%E0%B8%99%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B9%80%E0%B8%97%E0%B8%A8%E0%B9%84%E0%B8%97%E0%B8%A2&#34; target=&#34;_blank&#34;&gt;https://th.wikipedia.org/wiki/รายชื่อเทศบาลตำบลในประเทศไทย&lt;/a&gt;  เพื่อเก็บรายชื่อของเทศบาลตำบลทั้งหมดในประเทศไทย ก็อาจจะแบ่งได้เป็น 2 ขั้นตอน คือ Scrape และ Extract&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--plYMCEN0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/ov86mmsfcop0q86h867m.png&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://res.cloudinary.com/practicaldev/image/fetch/s--plYMCEN0--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/i/ov86mmsfcop0q86h867m.png&#34; alt=&#34;wikipedia-tambon.png&#34; /&gt;&lt;/a&gt;&lt;br /&gt;
&lt;em&gt;Wikipedia Page ที่จะทำการ scrape&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Scrape&lt;/strong&gt;  ขั้นตอนแรกก็คือต้องเอาข้อมูลจากหน้าเว็บแบบดิบ ๆ เนี่ย ออกมาให้ได้ก่อน ซึ่งในเว็บอื่น ๆ ทั่ว ๆ ไป ก็จะมีความซับซ้อนหลาย ๆ อย่างที่ทำให้ไม่สามารถดึงออกมาได้ง่าย ๆ ซึ่งจะนำเสนอเทคนิคต่าง ๆ ที่ใช้รับมือแต่ละ cases ในบทความถัด ๆ ไปนะครับ&lt;br /&gt;
แต่เนื่องจากในส่วนนี้เราใช้ Web Wikipedia เป็นตัวอย่าง ซึ่งไม่ได้มีการป้องการ scrape หรือกลไกที่ซับซ้อนอะไรมาก เพราะงั้นสามารถส่ง  &lt;code&gt;GET request&lt;/code&gt;  ไปที่ URL โดยตรงตามโค้ดด้านล่างได้เลยครับ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import requests

url = &#39;https://th.wikipedia.org/wiki/รายชื่อเทศบาลตำบลในประเทศไทย&#39;
resp = requests.get(url)
print(resp)

# Output: &amp;lt;Response [200]&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;จากโค้ดด้านบนเราจะได้ object ของ  &lt;code&gt;Response&lt;/code&gt;  มาเก็บไว้ในตัวแปร  &lt;code&gt;resp&lt;/code&gt;&lt;br /&gt;
โดยเราสามารถเรียกดู content ของ response ที่ได้กลับมาได้โดยใช้  &lt;code&gt;resp.content&lt;/code&gt;  หรือ  &lt;code&gt;resp.text&lt;/code&gt;  แตกต่างกันเล็กน้อย คือ  &lt;code&gt;.content&lt;/code&gt;  จะให้ค่า content ที่เป็น bytes ออกมา ส่วน  &lt;code&gt;.text&lt;/code&gt;  จะ decode ข้อมูล bytes ออกเป็น string ให้ จึงสามารถแสดงตัวอักษรภาษาต่าง ๆ ให้ดูได้ เช่นภาษาไทย&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;print(resp.text[:5_000])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;คำสั่งด้านบนจะเรียกดูข้อมูลจำนวน 5,000 ตัวอักษรแรก (limit ไว้เพื่อไม่ให้มันรกเฉย ๆ)&lt;/p&gt;

&lt;p&gt;จะเห็นว่าเราได้โค้ด HTML ของหน้า page นั้น ๆ มาทั้งหมด&lt;br /&gt;
นั่นคือตอนนี้เราดึงข้อมูลดิบของหน้าเว็บนั้นมาอยู่ใน Python เรียบร้อยแล้ว&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Extract&lt;/strong&gt;  ต่อไปเราจะนำข้อมูลดิบที่ได้มา มากรองให้เหลือแค่ข้อมูลที่เราอยากได้เท่านั้น นั่นก็คือชื่อเทศบาลตำบล&lt;br /&gt;
โดยทั่วไปในการ extract ข้อมูล วิธีการที่จะใช้นั้นก็ขึ้นอยู่กับหน้าตาของข้อมูล เช่น เป็น HTML, เป็น JSON, หรือเป็น plain text ซึ่ง modules ใน Python ที่จะนำมาใช้กับข้อมูลแต่ละประเภทก็มีหลายอย่าง เช่น  &lt;code&gt;lxml&lt;/code&gt;,  &lt;code&gt;json&lt;/code&gt;,  &lt;code&gt;beautifulsoup&lt;/code&gt;, หรือ  &lt;code&gt;re&lt;/code&gt;&lt;br /&gt;
ในกรณีนี้ เนื่องจากข้อมูลดิบเราเป็น HTML ผู้เขียนจึงใช้  &lt;code&gt;lxml&lt;/code&gt;  เนื่องจาก  &lt;code&gt;lxml&lt;/code&gt;  มีฟังก์ชันที่ใช้แปลงข้อมูล HTML ให้กลายเป็น tree ซึ่งช่วยให้เราสามารถ extract แต่ละ element ใน HTML ได้ง่าย (เป็นความเห็นส่วนตัวนะครับ บางท่านอาจจะถนัดใช้ beautifulsoup มากกว่า ซึ่งอาจจะเขียนถึงในบทความถัด ๆ ไป)&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;import lxml.etree

content = resp.content
tree = lxml.etree.fromstring(content, parser=lxml.etree.HTMLParser())
print(tree)

# Output: &amp;lt;Element html at 0x7f34c830e948&amp;gt;

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note: เราต้องระบุ  &lt;code&gt;parser&lt;/code&gt;  ให้เป็น  &lt;code&gt;HTMLParser&lt;/code&gt;  เนื่องจากจริง ๆ แล้ว  &lt;code&gt;lxml&lt;/code&gt;  สามารถใช้กับข้อมูลประเภทอื่นนอกจาก HTML ได้ด้วย&lt;/p&gt;

&lt;p&gt;พอเราแปลงข้อมูลเป็น tree แล้ว ต่อไปเราก็จะสามารถระบุตำแหน่งของ element ที่เก็บข้อมูลที่เราต้องการได้โดยใช้  &lt;code&gt;XPath&lt;/code&gt;&lt;br /&gt;
อธิบายแบบคร่าว ๆ  &lt;code&gt;XPath&lt;/code&gt;  ก็คล้าย ๆ กับ path ของ folder ต่าง ๆ ในเครื่องคอมพิวเตอร์ ที่เอาไว้ระบุตำแหน่งไฟล์หรือ folder ที่เราต้องการ โดยเริ่มจาก  &lt;em&gt;root&lt;/em&gt;  (C:/) แล้วก็ไปยัง folder ต่าง ๆ ตามลำดับ เช่น  &lt;em&gt;C:/Users/CopyPasteEng/Downloads&lt;/em&gt;  แต่  &lt;code&gt;XPath&lt;/code&gt;  ในที่นี้จะใช้ระบุตำแหน่งของ HTML element บนโค้ด code แทน&lt;br /&gt;
ตัวอย่าง  &lt;code&gt;XPath&lt;/code&gt;  เช่น  &lt;code&gt;//div[@id=&amp;quot;mw-content-text&amp;quot;]/div/p/text()&lt;/code&gt;  อันนี้จะแปลว่า&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1. ให้เข้าไปหา element ประเภท div ที่มี id เป็น &amp;quot;mw-content-text&amp;quot;
2. จากนั้นเข้าต่อไปที่ child element ที่เป็นประเภท div
3. แล้วก็เข้าต่อไปที่ element ประเภท p
4. แล้วเอา content ที่เป็น text ทั้งหมดออกมา

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;note:  &lt;code&gt;div[...]&lt;/code&gt;  คือการใส่เงื่อนไขในการเลือก  &lt;code&gt;div&lt;/code&gt;  นั้น ๆ เข้าไป ในกรณีด้านบน เงื่อนไขก็คือ attribute  &lt;em&gt;id&lt;/em&gt;  ต้องเท่ากับ  &lt;code&gt;mw-content-text&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;ทีนี้คำถามต่อมาก็คือจะเอา  &lt;code&gt;XPath&lt;/code&gt;  ที่บอกตำแหน่งของข้อมูลที่เราต้องการมาได้อย่างไร&lt;br /&gt;
คำตอบก็คือต้องดูจาก code ของ page ครับ ไม่มีทางอื่น 555&lt;br /&gt;
ซึ่งเครื่องมือหนึ่งที่สามารถช่วยให้แกะโค้ดของ web ต่าง ๆ ได้ง่าย ๆ และฟรี ก็คือ  &lt;strong&gt;Code Inspector&lt;/strong&gt;  บน  &lt;strong&gt;Google Chrome&lt;/strong&gt;  ครับ สามารถเปิดขึ้นมาได้โดยคลิกขวาที่หน้าเว็บแล้วเลือก  &lt;code&gt;Inspect&lt;/code&gt;  หรือกด  &lt;code&gt;F12&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;กลับมาที่เว็บตัวอย่างของเราครับ  &lt;code&gt;XPath&lt;/code&gt;  ของชื่อเทศบาลตำบลทั้งหมดที่เราต้องการก็คือ  &lt;code&gt;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[2 &amp;lt;= position()]/tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]//a//text()&lt;/code&gt;* เราจึงสามารถที่จะ extract ข้อมูลออกจาก tree ได้ด้วยคำสั่งต่อไปนี้ ก็จะได้ชื่อของเทศบาลตำบลในประเทศไทยจาก Wikipedia มาเก็บเป็น list เอาไว้ได้ตามต้องการครับ&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;xpath = &#39;//*[@id=&amp;quot;mw-content-text&amp;quot;]/div/table[2 &amp;lt;= position()]/tbody/tr/td[2 &amp;lt;= position() and position() &amp;lt;= 3]//a//text()&#39;
tambon_list = tree.xpath(xpath)
print(tambon_list)

# Output: [&#39;เทศบาลตำบลบ้านดู่&#39;, &#39;เทศบาลตำบลเวียงชัย&#39;, ...

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;*ขออนุญาตแก้ XPath ในตัวอย่างนี้เป็นแบบที่ยาวขึ้น เพื่อให้อธิบายได้ง่ายในบทความ part 2 นะครับ&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;เพื่อให้บทความกระชับ ผู้จัดทำได้ละรายละเอียดบางส่วนเอาไว้&lt;br /&gt;
เพราะจริง ๆ แล้ว ทั้งในส่วนของการ scrape และ extract ก็มีรายละเอียดอีกมาก และมีเทคนิคที่อาจจะต้องใช้มากมายซึ่งแตกต่างกันตามแต่ละ website เช่น  &lt;code&gt;XPath&lt;/code&gt;  ที่อยู่ใน code นี่ได้มาได้ยังไง? จะทำอย่างไรกับบาง website ที่มีกลไกที่ป้องกันการ scraping หรือบาง website ที่มี format ขอข้อมูลเป็นลักษณะอื่น&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;เพื่อให้ผู้อ่านได้รับข้อมูลครบถ้วน ในบทความนี้จึงนำเสนอเพียงแค่ basic ให้ผู้อ่านได้เห็นภาพรวมกว้าง ๆ ก่อน และคิดว่าจะกลับมาอธิบายลงลึกในแต่ละส่วนแยกกันไปอีกทีครับ&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;คิดว่าจากบทความนี้ ผู้อ่านน่าจะได้เห็นภาพของการ scrape แบบง่าย ๆ ไปแล้วสำหรับการเริ่มต้น ในบทความถัดไปเราก็จะสามารถไปดูเคสที่ซับซ้อนกันต่อได้ครับ&lt;/p&gt;

&lt;p&gt;อย่างที่ได้กล่าวไปข้างต้นครับ ว่าในการวิเคราะห์ข้อมูลเพื่อทำนายเหตุการณ์ต่าง ๆ ให้ถูกต้องและแม่นยำสิ่งที่สำคัญที่สุดคือ  &lt;strong&gt;ข้อมูล&lt;/strong&gt;  ที่อัพเดตตลอดเวลา&lt;br /&gt;
ซึ่งเราอาจจะมอง Internet เป็นเหมือนแหล่งข้อมูลฟรี ที่เราสามารถ scrape มายังไงก็ได้  &lt;strong&gt;แต่ควรคำนึงถึงเรื่องของกฎหมายและความเหมาะสมด้วยครับ สิ่งสำคัญก็คือต้องไม่ทำให้ใครเดือดร้อน&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;และสำหรับท่านที่สนใจอยากทราบรายละเอียดของแต่ละขั้นตอนอย่างละเอียด ก็สามารถติดตามได้ในบทความที่จะลงต่อ ๆ ไปนะครับ&lt;/p&gt;

&lt;p&gt;หรือถ้ามีเรื่องไหนที่สนใจเพิ่มเติมสามารถ comment เอาไว้ได้นะครับ&lt;/p&gt;

&lt;p&gt;FB Page:  &lt;a href=&#34;https://www.facebook.com/CopyPasteEng&#34; target=&#34;_blank&#34;&gt;Copy Paste Engineer&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Part อื่น ๆ ใน series&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-2/&#34;&gt;Part II - Chrome&amp;rsquo;s Code Inspector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-3/&#34;&gt;Part III - เทคนิคการ extract ข้อมูลด้วย XPath&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../python-web-scraping-part-4/&#34;&gt;Part IV - ทำไมถึง scrape บางเว็บไม่ได้&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;Reference : &lt;a href=&#34;https://dev.to/copypasteengineer/python-web-scraping-part-1-python-49ce&#34; target=&#34;_blank&#34;&gt;https://dev.to/copypasteengineer/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
